<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Evaluation And Tooling – STRAT 490R – Building Strategic AI Solutions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./09-business-strategy.html" rel="next">
<link href="./07-agents.html" rel="prev">
<link href="./images/strategic-ai-favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-5fd9db47a040b21ac2cac9a0b3b722ba.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating quarto-light"><header class="book-header-banner">
  <a href="https://byu-strategy.github.io/program-guide/" target="_blank">
    <img src="images/strat-logo.png" alt="BYU Strategy - Marriott School of Business">
  </a>
</header>


<link rel="stylesheet" href="styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-foundations.html">Topics</a></li><li class="breadcrumb-item"><a href="./08-evaluation-and-tooling.html"><span class="chapter-title">Evaluation And Tooling</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STRAT 490R – Building Strategic AI Solutions</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Syllabus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-schedule.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Schedule</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-assignments.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Assignments Overview</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-foundations.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Foundations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-prompt-engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Prompt Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-streamlit-ui.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Streamlit UI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-retrieval-augmented-generation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Retrieval Augmented Generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-fine-tuning.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Fine Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-agents.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agents</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-evaluation-and-tooling.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Evaluation And Tooling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-business-strategy.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Business Strategy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./90-resources.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./98-faq.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">FAQ</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99-references.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-ai-evaluation" id="toc-introduction-to-ai-evaluation" class="nav-link active" data-scroll-target="#introduction-to-ai-evaluation">Introduction to AI Evaluation</a></li>
  <li><a href="#importance-of-evaluation-in-ai-solutions" id="toc-importance-of-evaluation-in-ai-solutions" class="nav-link" data-scroll-target="#importance-of-evaluation-in-ai-solutions">Importance of Evaluation in AI Solutions</a></li>
  <li><a href="#key-metrics-for-evaluating-ai-models" id="toc-key-metrics-for-evaluating-ai-models" class="nav-link" data-scroll-target="#key-metrics-for-evaluating-ai-models">Key Metrics for Evaluating AI Models</a></li>
  <li><a href="#tools-for-model-evaluation" id="toc-tools-for-model-evaluation" class="nav-link" data-scroll-target="#tools-for-model-evaluation">Tools for Model Evaluation</a></li>
  <li><a href="#understanding-overfitting-and-underfitting" id="toc-understanding-overfitting-and-underfitting" class="nav-link" data-scroll-target="#understanding-overfitting-and-underfitting">Understanding Overfitting and Underfitting</a></li>
  <li><a href="#evaluation-in-the-context-of-rag-and-prompt-engineering" id="toc-evaluation-in-the-context-of-rag-and-prompt-engineering" class="nav-link" data-scroll-target="#evaluation-in-the-context-of-rag-and-prompt-engineering">Evaluation in the Context of RAG and Prompt Engineering</a></li>
  <li><a href="#continuous-monitoring-and-feedback-loops" id="toc-continuous-monitoring-and-feedback-loops" class="nav-link" data-scroll-target="#continuous-monitoring-and-feedback-loops">Continuous Monitoring and Feedback Loops</a></li>
  <li><a href="#debugging-and-error-analysis-techniques" id="toc-debugging-and-error-analysis-techniques" class="nav-link" data-scroll-target="#debugging-and-error-analysis-techniques">Debugging and Error Analysis Techniques</a></li>
  <li><a href="#the-confusion-matrix-helps-to-identify-specific-types-of-errors-such-as-false-positives-and-false-negatives." id="toc-the-confusion-matrix-helps-to-identify-specific-types-of-errors-such-as-false-positives-and-false-negatives." class="nav-link" data-scroll-target="#the-confusion-matrix-helps-to-identify-specific-types-of-errors-such-as-false-positives-and-false-negatives.">The confusion matrix helps to identify specific types of errors, such as false positives and false negatives.</a></li>
  <li><a href="#this-snippet-calculates-and-prints-the-permutation-importance-of-each-feature-helping-to-identify-which-features-the-model-relies-on-most." id="toc-this-snippet-calculates-and-prints-the-permutation-importance-of-each-feature-helping-to-identify-which-features-the-model-relies-on-most." class="nav-link" data-scroll-target="#this-snippet-calculates-and-prints-the-permutation-importance-of-each-feature-helping-to-identify-which-features-the-model-relies-on-most.">This snippet calculates and prints the permutation importance of each feature, helping to identify which features the model relies on most.</a></li>
  <li><a href="#lime-provides-a-visual-breakdown-of-the-contribution-of-each-feature-to-the-prediction-aiding-in-understanding-and-debugging." id="toc-lime-provides-a-visual-breakdown-of-the-contribution-of-each-feature-to-the-prediction-aiding-in-understanding-and-debugging." class="nav-link" data-scroll-target="#lime-provides-a-visual-breakdown-of-the-contribution-of-each-feature-to-the-prediction-aiding-in-understanding-and-debugging.">LIME provides a visual breakdown of the contribution of each feature to the prediction, aiding in understanding and debugging.</a>
  <ul class="collapse">
  <li><a href="#benchmarking-ai-models" id="toc-benchmarking-ai-models" class="nav-link" data-scroll-target="#benchmarking-ai-models">Benchmarking AI Models</a></li>
  <li><a href="#best-practices-for-evaluation-and-tooling" id="toc-best-practices-for-evaluation-and-tooling" class="nav-link" data-scroll-target="#best-practices-for-evaluation-and-tooling">Best Practices for Evaluation and Tooling</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-foundations.html">Topics</a></li><li class="breadcrumb-item"><a href="./08-evaluation-and-tooling.html"><span class="chapter-title">Evaluation And Tooling</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Evaluation And Tooling</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-ai-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-ai-evaluation">Introduction to AI Evaluation</h2>
<p>In the realm of artificial intelligence, evaluating the performance and effectiveness of AI solutions is as crucial as their development. Evaluation provides insights into how well an AI model performs, identifies areas for improvement, and ensures that the AI solution meets the desired objectives. This section introduces key concepts and methodologies for evaluating AI solutions, highlighting the importance of robust evaluation frameworks and tools.</p>
<p>AI evaluation can be broadly categorized into two types: quantitative and qualitative evaluation. Quantitative evaluation focuses on numerical metrics that objectively measure a model’s performance, such as accuracy, precision, recall, and F1-score. These metrics are particularly useful for comparing different models or configurations. On the other hand, qualitative evaluation involves subjective assessments, often through human judgment, to evaluate aspects like user experience or the ethical implications of an AI system.</p>
<p>Let’s delve into some common quantitative metrics used in AI evaluation. Accuracy is a straightforward metric that measures the proportion of correctly predicted instances over the total instances. However, in scenarios with imbalanced datasets, accuracy might be misleading. For example, in a dataset where 95% of the instances belong to one class, a model that predicts the majority class for all instances would achieve 95% accuracy yet fail to provide meaningful insights. In such cases, metrics like precision, recall, and F1-score become more informative.</p>
<div id="527d7ff8" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example predictions and true labels</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(true_labels, predictions)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(true_labels, predictions)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(true_labels, predictions)</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(true_labels, predictions)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">:.2f}</span><span class="ss">'</span>)  <span class="co"># Accuracy: 0.71</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Precision: </span><span class="sc">{</span>precision<span class="sc">:.2f}</span><span class="ss">'</span>)  <span class="co"># Precision: 0.75</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Recall: </span><span class="sc">{</span>recall<span class="sc">:.2f}</span><span class="ss">'</span>)  <span class="co"># Recall: 0.75</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'F1 Score: </span><span class="sc">{</span>f1<span class="sc">:.2f}</span><span class="ss">'</span>)  <span class="co"># F1 Score: 0.75</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the code example above, we demonstrate how to calculate key evaluation metrics using Python’s scikit-learn library. The <code>accuracy_score</code> function computes the accuracy, while <code>precision_score</code>, <code>recall_score</code>, and <code>f1_score</code> provide insights into the model’s precision, recall, and F1-score, respectively. These metrics help in understanding the trade-offs between false positives and false negatives, which is crucial in domains like medical diagnosis or fraud detection.</p>
<p>Beyond these basic metrics, more advanced evaluation techniques consider the context and specific requirements of the AI application. For instance, in natural language processing, BLEU and ROUGE scores are popular for evaluating machine translation and summarization tasks. In computer vision, Intersection over Union (IoU) is used to assess object detection models. The choice of evaluation metric should align with the problem’s goals and the stakeholders’ needs.</p>
<p>Qualitative evaluation, although less structured, is equally important. It involves understanding the user experience, ensuring the AI system behaves ethically, and assessing its impact on society. For example, human-in-the-loop evaluations can provide insights into how well AI systems assist humans in decision-making processes. Additionally, bias and fairness audits are essential to ensure that AI systems do not perpetuate or exacerbate existing inequalities.</p>
<p>In conclusion, evaluating AI solutions is a multifaceted process that requires a combination of quantitative and qualitative approaches. By employing the right evaluation metrics and methodologies, practitioners can ensure their AI solutions are not only effective but also fair and beneficial to society. In the following sections, we will explore specific tools and platforms that facilitate the evaluation of AI systems, providing practical insights into their implementation.</p>
</section>
<section id="importance-of-evaluation-in-ai-solutions" class="level2">
<h2 class="anchored" data-anchor-id="importance-of-evaluation-in-ai-solutions">Importance of Evaluation in AI Solutions</h2>
<p>In the development and deployment of AI solutions, evaluation plays a critical role. It is not merely a final step but an integral part of the AI lifecycle that influences design, development, and deployment decisions. Evaluation helps ensure that AI models meet the desired performance criteria and align with business objectives. More importantly, it provides insights into the strengths and weaknesses of a model, guiding iterative improvements and ensuring that the AI solution remains relevant and effective over time.</p>
<p>One of the primary reasons for evaluating AI solutions is to measure their performance against predefined metrics. These metrics can vary widely depending on the application and include accuracy, precision, recall, F1-score, and more for classification tasks, or mean squared error and R-squared for regression tasks. For instance, in a healthcare application predicting patient outcomes, high precision might be prioritized to avoid false positives that could lead to unnecessary treatments.</p>
<div id="4583267a" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score, f1_score</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># True labels</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]  <span class="co"># Predicted labels</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate precision, recall, and F1-score</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_true, y_pred)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_true, y_pred)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 Score: </span><span class="sc">{</span>f1<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Beyond numerical metrics, evaluation also involves assessing the model’s robustness, fairness, and interpretability. Robustness ensures that the model performs well under various conditions, such as different data distributions or noisy inputs. Fairness checks are crucial to ensure that AI solutions do not exhibit bias against any group. For example, a hiring algorithm should be evaluated for bias to ensure it provides equal opportunity regardless of gender, ethnicity, or age.</p>
<p>Interpretability is another key aspect of evaluation, especially in domains where understanding the model’s decision-making process is critical. Techniques such as SHAP (SHapley Additive exPlanations) or LIME (Local Interpretable Model-agnostic Explanations) can be used to provide insights into which features are driving the model’s predictions. This is particularly important in regulated industries like finance or healthcare, where transparency is mandatory.</p>
<div id="cdb3d1e6" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> shap</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Load a sample dataset</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> shap.datasets.boston()</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a simple XGBoost model</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> xgb.XGBRegressor().fit(X, y)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a SHAP explainer and get SHAP values</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> shap.Explainer(model, X)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>shap_values <span class="op">=</span> explainer(X)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Visualize the first prediction's explanation</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>shap.plots.waterfall(shap_values[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Finally, evaluation is not a one-time process but an ongoing one. As AI solutions are deployed and used, they encounter new data and scenarios. Continuous monitoring and evaluation are necessary to ensure that the AI continues to perform well and adapts to any changes in the environment or data distribution. This iterative process helps in maintaining the efficacy and reliability of AI solutions, thus maximizing their value to the organization.</p>
</section>
<section id="key-metrics-for-evaluating-ai-models" class="level2">
<h2 class="anchored" data-anchor-id="key-metrics-for-evaluating-ai-models">Key Metrics for Evaluating AI Models</h2>
<p>In evaluating AI models, selecting the right metrics is crucial to understanding the performance and reliability of the solution. Key metrics vary depending on the type of problem—classification, regression, clustering, etc.—and the specific goals of the AI system. This section will explore the most commonly used metrics for evaluating AI models and discuss their significance with examples.</p>
<p>For classification problems, accuracy is one of the most straightforward metrics. It measures the ratio of correctly predicted instances to the total instances. However, accuracy alone can be misleading, especially with imbalanced datasets where one class may dominate. For example, if 90% of the data belongs to one class, a model that predicts only that class will have 90% accuracy but is essentially useless.</p>
<div id="59a72f18" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_true, y_pred)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Output: Accuracy: 0.8333</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>To address the shortcomings of accuracy in imbalanced datasets, precision, recall, and F1 score are more informative. Precision measures the ratio of true positive predictions to the total predicted positives, indicating how many of the predicted positive cases were correct. Recall (or sensitivity) measures the ratio of true positive predictions to the actual positives, indicating how well the model identifies positive cases. The F1 score is the harmonic mean of precision and recall, providing a balance between the two.</p>
<div id="b65e6bc4" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score, f1_score</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_true, y_pred)</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_true, y_pred)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Precision: </span><span class="sc">{</span>precision<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Output: Precision: 1.0</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Recall: </span><span class="sc">{</span>recall<span class="sc">}</span><span class="ss">'</span>)        <span class="co"># Output: Recall: 0.6667</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'F1 Score: </span><span class="sc">{</span>f1<span class="sc">}</span><span class="ss">'</span>)         <span class="co"># Output: F1 Score: 0.8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In regression tasks, different metrics are used to evaluate model performance. Mean Absolute Error (MAE) and Mean Squared Error (MSE) are two common metrics. MAE measures the average magnitude of errors in a set of predictions, without considering their direction. MSE, on the other hand, squares the errors before averaging, which means it penalizes larger errors more than smaller ones.</p>
<div id="8b504bb5" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_absolute_error, mean_squared_error</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>true_values <span class="op">=</span> [<span class="fl">3.0</span>, <span class="op">-</span><span class="fl">0.5</span>, <span class="fl">2.0</span>, <span class="fl">7.0</span>]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> [<span class="fl">2.5</span>, <span class="fl">0.0</span>, <span class="fl">2.0</span>, <span class="fl">8.0</span>]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(true_values, predictions)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(true_values, predictions)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'MAE: </span><span class="sc">{</span>mae<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Output: MAE: 0.5</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'MSE: </span><span class="sc">{</span>mse<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Output: MSE: 0.375</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Root Mean Squared Error (RMSE) is another important metric in regression, representing the square root of MSE. RMSE is in the same units as the target variable, making it more interpretable. R-squared, or the coefficient of determination, measures how well the model’s predictions approximate the actual data points. An R-squared of 1 indicates perfect prediction, while 0 indicates that the model does no better than the mean of the target variable.</p>
<div id="894fbb3d" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>rmse <span class="op">=</span> np.sqrt(mean_squared_error(true_values, predictions))</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>r_squared <span class="op">=</span> r2_score(true_values, predictions)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'RMSE: </span><span class="sc">{</span>rmse<span class="sc">}</span><span class="ss">'</span>)      <span class="co"># Output: RMSE: 0.612372</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'R-squared: </span><span class="sc">{</span>r_squared<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Output: R-squared: 0.948608</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In clustering, metrics like Silhouette Score and Davies-Bouldin Index are used. The Silhouette Score measures how similar an object is to its own cluster compared to other clusters, with a score closer to 1 indicating better-defined clusters. The Davies-Bouldin Index evaluates the average similarity ratio of each cluster with its most similar cluster, where a lower value indicates better clustering.</p>
<div id="ca26e367" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score, davies_bouldin_score</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate sample data</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>X, _ <span class="op">=</span> make_blobs(n_samples<span class="op">=</span><span class="dv">100</span>, centers<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit KMeans</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> kmeans.fit_predict(X)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>silhouette_avg <span class="op">=</span> silhouette_score(X, labels)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>davies_bouldin <span class="op">=</span> davies_bouldin_score(X, labels)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Silhouette Score: </span><span class="sc">{</span>silhouette_avg<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Example output: Silhouette Score: 0.7</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Davies-Bouldin Index: </span><span class="sc">{</span>davies_bouldin<span class="sc">}</span><span class="ss">'</span>)  <span class="co"># Example output: Davies-Bouldin Index: 0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Choosing the right metric is fundamental to accurately assessing the performance of an AI model. Each metric provides different insights, and often, a combination of metrics is necessary to get a comprehensive view of a model’s performance. Understanding these metrics helps in optimizing models and ensuring they meet the strategic goals of the AI solution.</p>
</section>
<section id="tools-for-model-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="tools-for-model-evaluation">Tools for Model Evaluation</h2>
<p>In the realm of AI model evaluation, selecting the right tools is crucial for understanding model performance and ensuring that AI solutions are robust, reliable, and effective. These tools not only help in assessing how well a model performs but also provide insights into areas where the model might be improved. A comprehensive evaluation strategy typically involves using a combination of libraries and platforms that offer various features such as performance metrics computation, visualization, and error analysis.</p>
<p>One of the most widely used tools for model evaluation in Python is <code>scikit-learn</code>. This library provides a rich set of functions for calculating key performance metrics such as accuracy, precision, recall, and F1-score. It also offers utilities for generating confusion matrices and classification reports, which are essential for understanding the nuances of model performance across different classes. Let’s look at an example of how <code>scikit-learn</code> can be used to evaluate a classification model.</p>
<div id="2ea97527" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Example predictions and true labels</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>]</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate evaluation metrics</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> accuracy_score(y_true, y_pred)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_true, y_pred)</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_true, y_pred)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate confusion matrix</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>conf_matrix <span class="op">=</span> confusion_matrix(y_true, y_pred)</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate classification report</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>class_report <span class="op">=</span> classification_report(y_true, y_pred)</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 Score: </span><span class="sc">{</span>f1<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Confusion Matrix:</span><span class="ch">\n</span><span class="sc">{</span>conf_matrix<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Classification Report:</span><span class="ch">\n</span><span class="sc">{</span>class_report<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The above code snippet demonstrates how to compute various evaluation metrics using <code>scikit-learn</code>. These metrics provide a quantitative assessment of model performance. The confusion matrix, for example, offers a detailed breakdown of true positives, false positives, true negatives, and false negatives, which can be crucial for identifying specific areas where the model may be underperforming.</p>
<p>Another powerful tool for model evaluation is <code>TensorBoard</code>, which is part of the TensorFlow ecosystem. TensorBoard provides interactive visualizations that help track model metrics over time and analyze model behavior during training. This tool is particularly useful for deep learning models, where understanding the training process and identifying issues such as overfitting or vanishing gradients can be complex. TensorBoard’s visualization capabilities allow for a more intuitive understanding of these phenomena.</p>
<div id="53f099cf" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tensorflow.keras.callbacks <span class="im">import</span> TensorBoard</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assuming you have a model and data ready</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> ...  <span class="co"># your Keras model</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> ...   <span class="co"># your training data</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Set up TensorBoard callback</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>tensorboard_callback <span class="op">=</span> TensorBoard(log_dir<span class="op">=</span><span class="st">'./logs'</span>, histogram_freq<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model with the TensorBoard callback</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>model.fit(data, epochs<span class="op">=</span><span class="dv">10</span>, callbacks<span class="op">=</span>[tensorboard_callback])</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="co"># To visualize the logs, run the following command in your terminal:</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="co"># tensorboard --logdir=./logs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this code snippet, we see how to integrate TensorBoard into a Keras model training process. By specifying a log directory, TensorBoard will automatically record training metrics such as loss and accuracy, which can then be visualized in a web browser. This visualization helps in understanding how the model’s performance evolves over time and can be instrumental in diagnosing training issues.</p>
<p>Lastly, for more advanced evaluation needs, tools like <code>SHAP</code> and <code>LIME</code> are invaluable for model interpretability. These libraries help in understanding the decisions made by complex models by providing explanations for individual predictions. This is particularly important in domains where transparency and accountability are critical, such as healthcare and finance. By using these tools, practitioners can gain insights into which features are most influential in a model’s predictions, thus facilitating better decision-making and trust in AI solutions.</p>
</section>
<section id="understanding-overfitting-and-underfitting" class="level2">
<h2 class="anchored" data-anchor-id="understanding-overfitting-and-underfitting">Understanding Overfitting and Underfitting</h2>
<p>In the realm of machine learning and AI, understanding the concepts of overfitting and underfitting is crucial for creating models that generalize well to unseen data. These two phenomena are common pitfalls that can severely impact the performance of AI solutions if not properly addressed. To begin, let’s define these terms: overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise. As a result, it performs exceptionally well on the training data but poorly on new, unseen data. Underfitting, on the other hand, happens when a model is too simplistic to capture the underlying patterns in the data, resulting in poor performance on both the training and test datasets.</p>
<p>Imagine you’re tasked with predicting housing prices based on features such as the number of bedrooms, square footage, and location. An overfitted model might memorize the exact prices of the houses in your training data, including the random fluctuations unique to that dataset. Thus, when faced with new data, it struggles to make accurate predictions. Conversely, an underfitted model might only consider the average price of houses, ignoring the nuances provided by the features, and thus also fail to predict prices accurately.</p>
<div id="85964827" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate synthetic data</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">3</span> <span class="op">*</span> np.random.normal(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">100</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> X <span class="op">-</span> <span class="dv">2</span> <span class="op">*</span> (X <span class="op">**</span> <span class="dv">2</span>) <span class="op">+</span> np.random.normal(<span class="op">-</span><span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">100</span>)</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the data into training and test sets</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Reshape data</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train[:, np.newaxis]</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test[:, np.newaxis]</span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a linear model (underfitting example)</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>y_pred_train <span class="op">=</span> model.predict(X_train)</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>y_pred_test <span class="op">=</span> model.predict(X_test)</span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the mean squared error</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Underfitting - Train MSE:'</span>, mean_squared_error(y_train, y_pred_train))</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Underfitting - Test MSE:'</span>, mean_squared_error(y_test, y_pred_test))</span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">'gray'</span>, label<span class="op">=</span><span class="st">'Data'</span>)</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>plt.plot(X_train, y_pred_train, color<span class="op">=</span><span class="st">'red'</span>, label<span class="op">=</span><span class="st">'Linear Model'</span>)</span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Underfitting Example'</span>)</span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the code above, we generate synthetic data that follows a quadratic relationship. We then fit a simple linear regression model to this data. As expected, the linear model is unable to capture the quadratic nature of the data, resulting in underfitting. This is evident from the high mean squared error (MSE) on both the training and test datasets, as well as the poor visual fit of the model to the data.</p>
<div id="a9dc8abd" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Fit a polynomial model (potential overfitting example)</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>polynomial_features<span class="op">=</span> PolynomialFeatures(degree<span class="op">=</span><span class="dv">15</span>)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>X_train_poly <span class="op">=</span> polynomial_features.fit_transform(X_train)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>X_test_poly <span class="op">=</span> polynomial_features.transform(X_test)</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> LinearRegression()</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>model.fit(X_train_poly, y_train)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>y_pred_train_poly <span class="op">=</span> model.predict(X_train_poly)</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>y_pred_test_poly <span class="op">=</span> model.predict(X_test_poly)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate and print the mean squared error</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Overfitting - Train MSE:'</span>, mean_squared_error(y_train, y_pred_train_poly))</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'Overfitting - Test MSE:'</span>, mean_squared_error(y_test, y_pred_test_poly))</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot results</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>plt.scatter(X, y, color<span class="op">=</span><span class="st">'gray'</span>, label<span class="op">=</span><span class="st">'Data'</span>)</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>plt.scatter(X_train, y_pred_train_poly, color<span class="op">=</span><span class="st">'blue'</span>, label<span class="op">=</span><span class="st">'Polynomial Model'</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Overfitting Example'</span>)</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this example, we fit a polynomial regression model with a degree of 15 to the same dataset. This model is complex enough to capture the noise in the training data, leading to overfitting. While the training MSE is significantly lower, indicating a good fit to the training data, the test MSE is high, reflecting poor generalization to new data. The plot shows the model’s excessive complexity, which captures the noise rather than the true underlying pattern.</p>
<p>Balancing between overfitting and underfitting is key to developing robust AI solutions. Techniques such as cross-validation, regularization, and model selection based on validation performance are commonly employed to achieve this balance. Cross-validation helps ensure that the model’s performance is consistent across different subsets of the data, while regularization techniques, like Lasso or Ridge regression, add a penalty for model complexity, discouraging overfitting. Selecting the right model complexity, often guided by domain knowledge and empirical testing, is crucial for achieving the best performance in real-world applications.</p>
</section>
<section id="evaluation-in-the-context-of-rag-and-prompt-engineering" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-in-the-context-of-rag-and-prompt-engineering">Evaluation in the Context of RAG and Prompt Engineering</h2>
<p>In the realm of AI solutions, particularly those involving Retrieval-Augmented Generation (RAG) and prompt engineering, evaluation plays a pivotal role in ensuring the effectiveness and reliability of the models. Unlike traditional AI systems, where evaluation metrics may focus solely on accuracy or precision, RAG and prompt-based systems require a more nuanced approach. This is because these systems often involve a combination of information retrieval and natural language generation, each with its own set of challenges and evaluation criteria.</p>
<p>RAG systems integrate retrieval mechanisms with generative models to produce responses that are both contextually relevant and factually accurate. Evaluation in this context involves assessing the quality of both the retrieval and the generation components. For retrieval, precision and recall are critical metrics, as they measure the system’s ability to find relevant information from a large corpus. For generation, metrics like BLEU, ROUGE, or METEOR might be used to evaluate the quality of the generated text against reference outputs.</p>
<p>Prompt engineering, on the other hand, involves designing input prompts that elicit the desired behavior from a language model. Evaluating prompt effectiveness requires an understanding of how different prompts influence model outputs, and may involve both quantitative metrics and qualitative assessments. Quantitative metrics could include response relevance or coherence scores, while qualitative assessments might involve human evaluators rating the outputs based on criteria like informativeness or creativity.</p>
<div id="442a35a7" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of evaluating a RAG system</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assume we have a list of true and predicted retrieval outputs</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>true_retrievals <span class="op">=</span> [[<span class="st">'doc1'</span>, <span class="st">'doc3'</span>], [<span class="st">'doc2'</span>], [<span class="st">'doc4'</span>, <span class="st">'doc5'</span>]]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>predicted_retrievals <span class="op">=</span> [[<span class="st">'doc1'</span>, <span class="st">'doc2'</span>], [<span class="st">'doc2'</span>], [<span class="st">'doc4'</span>, <span class="st">'doc6'</span>]]</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Flatten the lists for metric calculation</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>true_flat <span class="op">=</span> [doc <span class="cf">for</span> docs <span class="kw">in</span> true_retrievals <span class="cf">for</span> doc <span class="kw">in</span> docs]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>predicted_flat <span class="op">=</span> [doc <span class="cf">for</span> docs <span class="kw">in</span> predicted_retrievals <span class="cf">for</span> doc <span class="kw">in</span> docs]</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate precision and recall</span></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(true_flat, predicted_flat, average<span class="op">=</span><span class="st">'micro'</span>)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(true_flat, predicted_flat, average<span class="op">=</span><span class="st">'micro'</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Precision: </span><span class="sc">{</span>precision<span class="sc">:.2f}</span><span class="ss">'</span>)</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Recall: </span><span class="sc">{</span>recall<span class="sc">:.2f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the above code example, we simulate the evaluation of a RAG system’s retrieval component. We use precision and recall to assess how well the system retrieves relevant documents compared to a ground truth set. This evaluation is crucial because the quality of the retrieved documents directly impacts the quality of the generated output.</p>
<p>For prompt engineering, the evaluation process often involves iterative testing and refinement. A prompt that works well in one context might not perform as expected in another, due to the inherent variability in language models. Therefore, prompt evaluation is typically an exploratory process, where different prompts are tested and their outputs analyzed for alignment with the desired outcome.</p>
<div id="71671f58" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of evaluating prompt responses</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> pipeline</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize a text generation model</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>generator <span class="op">=</span> pipeline(<span class="st">'text-generation'</span>, model<span class="op">=</span><span class="st">'gpt2'</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Define different prompts</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>prompts <span class="op">=</span> [</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Explain the theory of relativity in simple terms."</span>,</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">"What are the key principles of the theory of relativity?"</span>,</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Summarize the theory of relativity for a young audience."</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate responses and evaluate</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> prompt <span class="kw">in</span> prompts:</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    response <span class="op">=</span> generator(prompt, max_length<span class="op">=</span><span class="dv">50</span>, num_return_sequences<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Response: </span><span class="sc">{</span>response[<span class="dv">0</span>][<span class="st">'generated_text'</span>]<span class="sc">}</span></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a><span class="er">')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this code example, we utilize a pre-trained language model to generate responses to different prompts. The responses are then qualitatively assessed for relevance, coherence, and alignment with the prompt’s intent. This hands-on approach allows practitioners to iteratively refine prompts and improve the overall performance of AI systems in generating useful and accurate information.</p>
</section>
<section id="continuous-monitoring-and-feedback-loops" class="level2">
<h2 class="anchored" data-anchor-id="continuous-monitoring-and-feedback-loops">Continuous Monitoring and Feedback Loops</h2>
<p>In the rapidly evolving field of artificial intelligence, particularly in applications like Retrieval-Augmented Generation (RAG) and prompt engineering, continuous monitoring and feedback loops are critical components. These processes ensure that AI solutions not only maintain their performance over time but also adapt to new data and changing environments. Continuous monitoring involves the regular observation of an AI system’s performance metrics, while feedback loops provide mechanisms for automatically adjusting the system based on new information.</p>
<p>Continuous monitoring is essential for identifying when an AI model’s performance begins to degrade. This degradation can occur due to data drift, where the statistical properties of the input data change over time, or concept drift, where the underlying relationships that the model has learned change. For example, a sentiment analysis model trained on social media posts might perform well initially but could become less accurate if the language or topics discussed by users evolve over time. By continuously monitoring metrics such as accuracy, precision, recall, and F1-score, developers can quickly identify when a model needs retraining or adjustment.</p>
<div id="217ffa56" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated function to get new data and predictions</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co"># This would be replaced by actual data retrieval and model prediction logic</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_new_data_and_predictions():</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Simulate new data and predictions</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># In practice, replace this with actual data fetching and model prediction</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>], [<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>]  <span class="co"># true_labels, predicted_labels</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Continuous monitoring function</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> monitor_model_performance(interval<span class="op">=</span><span class="dv">60</span>):</span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>        true_labels, predicted_labels <span class="op">=</span> get_new_data_and_predictions()</span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>        accuracy <span class="op">=</span> accuracy_score(true_labels, predicted_labels)</span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Current accuracy: </span><span class="sc">{</span>accuracy<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Add logic to trigger retraining if accuracy drops below a threshold</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> accuracy <span class="op">&lt;</span> <span class="fl">0.8</span>:</span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">"Warning: Model performance has degraded. Consider retraining."</span>)</span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>        time.sleep(interval)</span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Start monitoring with a 60-second interval</span></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>monitor_model_performance()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Feedback loops are mechanisms that allow AI systems to learn from their mistakes and improve over time. In the context of RAG and prompt engineering, feedback loops can be used to refine retrieval strategies or modify prompts based on user interactions and outcomes. For instance, if a chatbot consistently fails to provide relevant answers to user queries, a feedback loop might involve analyzing these interactions to identify patterns and adjust the retrieval strategy or prompt templates accordingly.</p>
<p>A practical implementation of a feedback loop might involve logging user interactions and model responses, then using this data to update the model or its parameters. This process can be automated using techniques such as reinforcement learning, where the system receives rewards or penalties based on its performance and adjusts its behavior to maximize positive outcomes. Consider a scenario where a recommendation system suggests products to users. If users frequently ignore certain recommendations, a feedback loop might penalize these suggestions and explore alternative options.</p>
<div id="7d76ff32" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Simulated user interaction log</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_user_interaction(user_id, interaction, success):</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># This would store interactions in a database or file in a real system</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Logging interaction for user </span><span class="sc">{</span>user_id<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>interaction<span class="sc">}</span><span class="ss">, success: </span><span class="sc">{</span>success<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Feedback loop function</span></span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> feedback_loop(user_interactions):</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    feedback_scores <span class="op">=</span> defaultdict(<span class="bu">int</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> user_id, interaction, success <span class="kw">in</span> user_interactions:</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>        log_user_interaction(user_id, interaction, success)</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update feedback score based on success</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        feedback_scores[interaction] <span class="op">+=</span> <span class="dv">1</span> <span class="cf">if</span> success <span class="cf">else</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Adjust system parameters based on feedback scores</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> interaction, score <span class="kw">in</span> feedback_scores.items():</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> score <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Consider revising strategy for interaction: </span><span class="sc">{</span>interaction<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co"># Example user interactions</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>user_interactions <span class="op">=</span> [</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="st">'recommendation_A'</span>, <span class="va">False</span>),</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="st">'recommendation_B'</span>, <span class="va">True</span>),</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="st">'recommendation_A'</span>, <span class="va">False</span>),</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="st">'recommendation_C'</span>, <span class="va">True</span>)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Run feedback loop</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>feedback_loop(user_interactions)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In summary, continuous monitoring and feedback loops are indispensable for maintaining and improving AI solutions. They provide the necessary infrastructure to detect performance issues early and adapt to new challenges, ensuring that AI systems remain robust and effective over time. By implementing these processes, organizations can enhance the reliability and relevance of their AI applications, ultimately leading to better decision-making and user satisfaction.</p>
</section>
<section id="debugging-and-error-analysis-techniques" class="level2">
<h2 class="anchored" data-anchor-id="debugging-and-error-analysis-techniques">Debugging and Error Analysis Techniques</h2>
<p>In the realm of AI solutions, debugging and error analysis are critical components that ensure the reliability and effectiveness of models. Unlike traditional software debugging, AI debugging often involves understanding the complex interactions between data, model architecture, and algorithms. This section will delve into various techniques and tools that can be employed to identify and rectify issues in AI systems, enhancing their performance and reliability.</p>
<p>One of the primary techniques in AI debugging is the analysis of model outputs to identify patterns of errors. This involves examining the predictions made by the model and comparing them to the ground truth to identify systematic errors. For instance, if a model consistently misclassifies a particular class, it might indicate a need for more training data for that class or a problem with feature representation.</p>
<div id="c1005ba9" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Assume y_true and y_pred are the true and predicted labels respectively</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>])</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> np.array([<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>])</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute confusion matrix</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>cm <span class="op">=</span> confusion_matrix(y_true, y_pred)</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Confusion Matrix:"</span>)</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(cm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-confusion-matrix-helps-to-identify-specific-types-of-errors-such-as-false-positives-and-false-negatives." class="level1">
<h1>The confusion matrix helps to identify specific types of errors, such as false positives and false negatives.</h1>
<p>Another essential aspect of debugging AI solutions is feature importance analysis. This involves understanding which features are most influential in the model’s decision-making process. Techniques such as permutation importance and SHAP (SHapley Additive exPlanations) values can be used to identify and interpret feature importance, which can highlight potential issues with feature selection or data preprocessing.</p>
<div id="41f7267d" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.inspection <span class="im">import</span> permutation_importance</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Train a simple Random Forest model</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> RandomForestClassifier()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>model.fit(X_train, y_train)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate feature importance using permutation importance</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> permutation_importance(model, X_test, y_test, n_repeats<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display feature importances</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> result.importances_mean.argsort()[::<span class="op">-</span><span class="dv">1</span>]:</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Feature </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>result<span class="sc">.</span>importances_mean[i]<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="this-snippet-calculates-and-prints-the-permutation-importance-of-each-feature-helping-to-identify-which-features-the-model-relies-on-most." class="level1">
<h1>This snippet calculates and prints the permutation importance of each feature, helping to identify which features the model relies on most.</h1>
<p>Error analysis can also be enhanced by visualizing model decisions. Tools like LIME (Local Interpretable Model-agnostic Explanations) can be used to generate visual explanations for individual predictions, providing insights into how the model arrived at a particular decision. This can be particularly useful in identifying cases where the model is overfitting to noise or irrelevant features.</p>
<div id="bdd8db0c" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lime</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> lime.lime_tabular</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a LIME explainer</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>explainer <span class="op">=</span> lime.lime_tabular.LimeTabularExplainer(</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    X_train, feature_names<span class="op">=</span>feature_names, class_names<span class="op">=</span>class_names, discretize_continuous<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Explain a single prediction</span></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>exp <span class="op">=</span> explainer.explain_instance(X_test[<span class="dv">0</span>], model.predict_proba, num_features<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Display the explanation</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>exp.show_in_notebook(show_table<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="lime-provides-a-visual-breakdown-of-the-contribution-of-each-feature-to-the-prediction-aiding-in-understanding-and-debugging." class="level1">
<h1>LIME provides a visual breakdown of the contribution of each feature to the prediction, aiding in understanding and debugging.</h1>
<p>Finally, leveraging automated tools and frameworks for error analysis can significantly streamline the debugging process. Frameworks like TensorBoard for TensorFlow or Weights &amp; Biases offer comprehensive visualization and tracking capabilities, allowing developers to monitor metrics, visualize model architecture, and trace errors back to their source. These tools can be invaluable for maintaining an efficient debugging workflow in complex AI projects.</p>
<section id="benchmarking-ai-models" class="level2">
<h2 class="anchored" data-anchor-id="benchmarking-ai-models">Benchmarking AI Models</h2>
<p>Benchmarking AI models is a critical step in the lifecycle of developing AI solutions. It involves evaluating the performance of models against a set of standardized metrics and datasets to ensure they meet the required standards for deployment. Benchmarking provides a clear understanding of how well a model performs in comparison to other models and helps identify areas for improvement. This process is essential for making informed decisions about model selection and deployment strategies.</p>
<p>When benchmarking AI models, it’s important to consider several key metrics. For classification tasks, common metrics include accuracy, precision, recall, F1 score, and the area under the ROC curve (AUC-ROC). For regression tasks, metrics such as mean squared error (MSE), mean absolute error (MAE), and R-squared are often used. These metrics provide insights into different aspects of model performance, such as how well the model predicts positive cases or how closely the model’s predictions match the actual values.</p>
<p>To illustrate the benchmarking process, let’s consider a classification problem where we have trained multiple models to predict whether an email is spam or not. We will use Python and some common libraries to evaluate these models based on accuracy, precision, recall, and F1 score. This example will demonstrate how to implement a basic benchmarking process using a synthetic dataset.</p>
<div id="53079fcf" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestClassifier</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score, precision_score, recall_score, f1_score</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a synthetic dataset</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_classification(n_samples<span class="op">=</span><span class="dv">1000</span>, n_features<span class="op">=</span><span class="dv">20</span>, n_informative<span class="op">=</span><span class="dv">2</span>, n_redundant<span class="op">=</span><span class="dv">10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into training and testing sets</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(X, y, test_size<span class="op">=</span><span class="fl">0.3</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the models</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>rf_model <span class="op">=</span> RandomForestClassifier(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>svm_model <span class="op">=</span> SVC(random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the models</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>rf_model.fit(X_train, y_train)</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>svm_model.fit(X_train, y_train)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict with the models</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>rf_predictions <span class="op">=</span> rf_model.predict(X_test)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>svm_predictions <span class="op">=</span> svm_model.predict(X_test)</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to evaluate models</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_model(predictions, y_true):</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    accuracy <span class="op">=</span> accuracy_score(y_true, predictions)</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> precision_score(y_true, predictions)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> recall_score(y_true, predictions)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    f1 <span class="op">=</span> f1_score(y_true, predictions)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> accuracy, precision, recall, f1</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the Random Forest model</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>rf_metrics <span class="op">=</span> evaluate_model(rf_predictions, y_test)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Random Forest - Accuracy: </span><span class="sc">{</span>rf_metrics[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">, Precision: </span><span class="sc">{</span>rf_metrics[<span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">, Recall: </span><span class="sc">{</span>rf_metrics[<span class="dv">2</span>]<span class="sc">:.2f}</span><span class="ss">, F1 Score: </span><span class="sc">{</span>rf_metrics[<span class="dv">3</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluate the SVM model</span></span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>svm_metrics <span class="op">=</span> evaluate_model(svm_predictions, y_test)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"SVM - Accuracy: </span><span class="sc">{</span>svm_metrics[<span class="dv">0</span>]<span class="sc">:.2f}</span><span class="ss">, Precision: </span><span class="sc">{</span>svm_metrics[<span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">, Recall: </span><span class="sc">{</span>svm_metrics[<span class="dv">2</span>]<span class="sc">:.2f}</span><span class="ss">, F1 Score: </span><span class="sc">{</span>svm_metrics[<span class="dv">3</span>]<span class="sc">:.2f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the code example above, we first create a synthetic dataset using <code>make_classification</code>, which simulates a binary classification problem. We then split the dataset into training and testing sets. Two different models, a Random Forest and a Support Vector Machine (SVM), are trained on the training data. After training, we predict the test data and evaluate the models using a set of metrics: accuracy, precision, recall, and F1 score. These metrics provide a comprehensive view of each model’s performance, allowing us to compare them effectively.</p>
<p>Benchmarking is not only about comparing models but also about understanding the trade-offs between different metrics. For example, a model with high accuracy might have low precision and recall if the dataset is imbalanced. Therefore, it’s crucial to select metrics that align with the specific goals of your AI solution. Additionally, benchmarking should be an iterative process, where models are continuously evaluated and improved based on the feedback from these metrics.</p>
<p>Finally, benchmarking should also consider the computational efficiency and scalability of models, especially when deploying AI solutions in production environments. This includes evaluating the time complexity and resource usage of models during training and inference. By incorporating these considerations, you can ensure that your AI solutions are not only effective but also practical for real-world applications.</p>
</section>
<section id="best-practices-for-evaluation-and-tooling" class="level2">
<h2 class="anchored" data-anchor-id="best-practices-for-evaluation-and-tooling">Best Practices for Evaluation and Tooling</h2>
<p>In the development of AI solutions, evaluation and tooling are critical components that ensure the effectiveness and reliability of models. Evaluation involves assessing the performance of AI models using various metrics, while tooling refers to the ecosystem of software and frameworks that support the development, deployment, and maintenance of AI systems. By adhering to best practices in both areas, organizations can build robust AI solutions that meet their strategic goals.</p>
<p>One of the fundamental best practices in evaluation is the use of appropriate metrics that align with the business objectives. For instance, in a classification task, accuracy might be a straightforward metric, but it may not always reflect the true performance of a model, especially in imbalanced datasets where precision, recall, and F1-score become more relevant. For regression tasks, metrics such as Mean Absolute Error (MAE) or Root Mean Square Error (RMSE) provide insights into the model’s prediction capabilities. Selecting the right metric is crucial as it directly impacts how the model’s success is defined and perceived.</p>
<p>Consider a scenario where you are developing a spam detection system. Here, the cost of false positives (legitimate emails marked as spam) might be higher than false negatives (spam emails not detected). In such cases, precision is a more critical metric than recall. This example highlights the importance of understanding the context and consequences of errors when choosing evaluation metrics.</p>
<div id="04076c28" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> precision_score, recall_score, f1_score</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>y_true <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>]  <span class="co"># True labels</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>y_pred <span class="op">=</span> [<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>]  <span class="co"># Predicted labels</span></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate precision, recall, and F1-score</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> precision_score(y_true, y_pred)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> recall_score(y_true, y_pred)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(y_true, y_pred)</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span>precision<span class="sc">:.2f}</span><span class="ss">"</span>)  <span class="co"># Precision: 0.75</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span>recall<span class="sc">:.2f}</span><span class="ss">"</span>)        <span class="co"># Recall: 0.60</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 Score: </span><span class="sc">{</span>f1<span class="sc">:.2f}</span><span class="ss">"</span>)          <span class="co"># F1 Score: 0.67</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Tooling, on the other hand, encompasses the frameworks and environments that facilitate the entire lifecycle of AI models, from development to deployment. Best practices in tooling involve using well-maintained libraries and frameworks that are widely supported by the community. For example, TensorFlow and PyTorch are popular choices for deep learning tasks due to their extensive documentation and active user communities.</p>
<p>Version control is another critical aspect of tooling. By using version control systems like Git, teams can track changes in code, collaborate efficiently, and maintain a history of model iterations. This practice is especially important in AI projects where reproducibility is key. Furthermore, integrating Continuous Integration/Continuous Deployment (CI/CD) pipelines ensures that models are automatically tested and deployed, reducing the risk of human error and speeding up the development process.</p>
<div id="5747cf36" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Example of a simple CI/CD pipeline configuration using GitHub Actions</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>yaml_content <span class="op">=</span> <span class="st">'''</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="st">name: CI/CD</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="st">on: [push]</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="st">jobs:</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a><span class="st">  build:</span></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="st">    runs-on: ubuntu-latest</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="st">    steps:</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="st">    - uses: actions/checkout@v2</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a><span class="st">    - name: Set up Python 3.8</span></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a><span class="st">      uses: actions/setup-python@v2</span></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="st">      with:</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="st">        python-version: 3.8</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="st">    - name: Install dependencies</span></span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="st">      run: |</span></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="st">        python -m pip install --upgrade pip</span></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a><span class="st">        pip install -r requirements.txt</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a><span class="st">    - name: Run tests</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a><span class="st">      run: |</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a><span class="st">        pytest test_suite</span></span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a><span class="st">'''</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Save the YAML configuration to a file</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'.github/workflows/ci-cd.yml'</span>, <span class="st">'w'</span>) <span class="im">as</span> <span class="bu">file</span>:</span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>    <span class="bu">file</span>.write(yaml_content)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In conclusion, the best practices for evaluation and tooling in AI solutions involve a careful selection of metrics that align with business objectives, the use of robust and community-supported frameworks, and the implementation of systems that ensure reproducibility and efficiency in model development and deployment. By integrating these practices, organizations can enhance the quality and impact of their AI initiatives.</p>


</section>
</section>

</main> <!-- /main -->
<footer class="book-footer">
  <div class="footer-content">
    <div class="footer-left">
      <img src="images/strat-logo.png" alt="BYU Logo" class="footer-logo">
    </div>
    <div class="footer-right">
      <a href="https://www.scottmurff.org" target="_blank">Scott Murff</a>
      |
      <a href="https://www.linkedin.com/in/scottdmurff" target="_blank">LinkedIn</a>
    </div>
  </div>
</footer>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./07-agents.html" class="pagination-link" aria-label="Agents">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Agents</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./09-business-strategy.html" class="pagination-link" aria-label="Business Strategy">
        <span class="nav-page-text"><span class="chapter-title">Business Strategy</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>