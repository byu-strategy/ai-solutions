<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.33">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Fine Tuning – STRAT 490R – Building Strategic AI Solutions</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./07-agents.html" rel="next">
<link href="./05-retrieval-augmented-generation.html" rel="prev">
<link href="./images/strategic-ai-favicon.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-ea385d0e468b0dd5ea5bf0780b1290d9.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-485d01fc63b59abcd3ee1bf1e8e2748d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
</head><body class="nav-sidebar floating quarto-light"><header class="book-header-banner">
  <a href="https://byu-strategy.github.io/program-guide/" target="_blank">
    <img src="images/strat-logo.png" alt="BYU Strategy - Marriott School of Business">
  </a>
</header>
<meta name="mermaid-theme" content="neutral">
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">


<link rel="stylesheet" href="styles.css">




<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-foundations.html">Topics</a></li><li class="breadcrumb-item"><a href="./06-fine-tuning.html"><span class="chapter-title">Fine Tuning</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STRAT 490R – Building Strategic AI Solutions</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Course Information</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Syllabus</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-schedule.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Schedule</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./01-assignments.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Assignments Overview</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Topics</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./02-foundations.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Foundations</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./03-prompt-engineering.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Prompt Engineering</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./04-streamlit-ui.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Streamlit UI</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./05-retrieval-augmented-generation.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Retrieval Augmented Generation</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./06-fine-tuning.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Fine Tuning</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./07-agents.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Agents</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./08-evaluation-and-tooling.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Evaluation And Tooling</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./09-business-strategy.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Business Strategy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./90-resources.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Resources</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./98-faq.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">FAQ</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./99-references.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">References</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction-to-model-fine-tuning" id="toc-introduction-to-model-fine-tuning" class="nav-link active" data-scroll-target="#introduction-to-model-fine-tuning">Introduction to Model Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#what-is-fine-tuning" id="toc-what-is-fine-tuning" class="nav-link" data-scroll-target="#what-is-fine-tuning">What is Fine-Tuning?</a></li>
  <li><a href="#benefits-and-uses-of-fine-tuning" id="toc-benefits-and-uses-of-fine-tuning" class="nav-link" data-scroll-target="#benefits-and-uses-of-fine-tuning">Benefits and Uses of Fine-Tuning</a></li>
  <li><a href="#challenges-and-limitations-of-fine-tuning" id="toc-challenges-and-limitations-of-fine-tuning" class="nav-link" data-scroll-target="#challenges-and-limitations-of-fine-tuning">Challenges and Limitations of Fine-Tuning</a></li>
  <li><a href="#how-fine-tuning-works-a-roadmap" id="toc-how-fine-tuning-works-a-roadmap" class="nav-link" data-scroll-target="#how-fine-tuning-works-a-roadmap">How Fine-Tuning Works: A Roadmap</a></li>
  </ul></li>
  <li><a href="#choose-a-base-model" id="toc-choose-a-base-model" class="nav-link" data-scroll-target="#choose-a-base-model">1) Choose a Base Model</a>
  <ul class="collapse">
  <li><a href="#model-size-number-of-parameters" id="toc-model-size-number-of-parameters" class="nav-link" data-scroll-target="#model-size-number-of-parameters"><strong>Model Size (Number of Parameters)</strong></a></li>
  <li><a href="#pre-trained-vs-instruction-tuned" id="toc-pre-trained-vs-instruction-tuned" class="nav-link" data-scroll-target="#pre-trained-vs-instruction-tuned"><strong>Pre-Trained vs Instruction Tuned</strong></a></li>
  <li><a href="#licensing-and-commercial-use" id="toc-licensing-and-commercial-use" class="nav-link" data-scroll-target="#licensing-and-commercial-use"><strong>Licensing and Commercial Use</strong></a></li>
  <li><a href="#model-compatibility-and-alignment" id="toc-model-compatibility-and-alignment" class="nav-link" data-scroll-target="#model-compatibility-and-alignment"><strong>Model Compatibility and Alignment</strong></a></li>
  </ul></li>
  <li><a href="#data-preparation-for-fine-tuning" id="toc-data-preparation-for-fine-tuning" class="nav-link" data-scroll-target="#data-preparation-for-fine-tuning">2) Data Preparation for Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#data-sourcing" id="toc-data-sourcing" class="nav-link" data-scroll-target="#data-sourcing"><strong>Data Sourcing</strong></a></li>
  <li><a href="#data-cleaning" id="toc-data-cleaning" class="nav-link" data-scroll-target="#data-cleaning"><strong>Data Cleaning</strong></a></li>
  <li><a href="#data-formatting" id="toc-data-formatting" class="nav-link" data-scroll-target="#data-formatting"><strong>Data Formatting</strong></a></li>
  </ul></li>
  <li><a href="#techniques-and-tools-for-fine-tuning" id="toc-techniques-and-tools-for-fine-tuning" class="nav-link" data-scroll-target="#techniques-and-tools-for-fine-tuning">3) Techniques and Tools for Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#fine-tuning-techniques" id="toc-fine-tuning-techniques" class="nav-link" data-scroll-target="#fine-tuning-techniques"><strong>Fine-Tuning Techniques</strong></a></li>
  <li><a href="#fine-tuning-tools-no-code-to-low-code" id="toc-fine-tuning-tools-no-code-to-low-code" class="nav-link" data-scroll-target="#fine-tuning-tools-no-code-to-low-code"><strong>Fine-Tuning Tools (No-Code to Low-Code)</strong></a></li>
  </ul></li>
  <li><a href="#training-arguments-and-hyperparameter-optimization" id="toc-training-arguments-and-hyperparameter-optimization" class="nav-link" data-scroll-target="#training-arguments-and-hyperparameter-optimization">4) Training Arguments and Hyperparameter Optimization</a>
  <ul class="collapse">
  <li><a href="#learning-rate" id="toc-learning-rate" class="nav-link" data-scroll-target="#learning-rate"><strong>1) Learning Rate</strong></a></li>
  <li><a href="#number-of-epochs" id="toc-number-of-epochs" class="nav-link" data-scroll-target="#number-of-epochs"><strong>2) Number of Epochs</strong></a></li>
  <li><a href="#batch-size" id="toc-batch-size" class="nav-link" data-scroll-target="#batch-size"><strong>3) Batch Size</strong></a></li>
  <li><a href="#gradient-accumulation-steps" id="toc-gradient-accumulation-steps" class="nav-link" data-scroll-target="#gradient-accumulation-steps"><strong>4) Gradient Accumulation Steps</strong></a></li>
  <li><a href="#cutoff-length" id="toc-cutoff-length" class="nav-link" data-scroll-target="#cutoff-length"><strong>5) Cutoff Length</strong></a></li>
  <li><a href="#validation-size" id="toc-validation-size" class="nav-link" data-scroll-target="#validation-size"><strong>6) Validation Size</strong></a></li>
  </ul></li>
  <li><a href="#evaluating-training-with-the-loss-curve" id="toc-evaluating-training-with-the-loss-curve" class="nav-link" data-scroll-target="#evaluating-training-with-the-loss-curve">5) Evaluating Training with the Loss Curve</a>
  <ul class="collapse">
  <li><a href="#understanding-loss" id="toc-understanding-loss" class="nav-link" data-scroll-target="#understanding-loss">Understanding Loss</a></li>
  <li><a href="#underfitting" id="toc-underfitting" class="nav-link" data-scroll-target="#underfitting">Underfitting</a></li>
  <li><a href="#overfitting" id="toc-overfitting" class="nav-link" data-scroll-target="#overfitting">Overfitting</a></li>
  <li><a href="#a-healthy-fit-good-generalization" id="toc-a-healthy-fit-good-generalization" class="nav-link" data-scroll-target="#a-healthy-fit-good-generalization">A Healthy Fit (Good Generalization)</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./02-foundations.html">Topics</a></li><li class="breadcrumb-item"><a href="./06-fine-tuning.html"><span class="chapter-title">Fine Tuning</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Fine Tuning</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction-to-model-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="introduction-to-model-fine-tuning">Introduction to Model Fine-Tuning</h2>
<section id="what-is-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="what-is-fine-tuning">What is Fine-Tuning?</h3>
<p>To understand fine-tuning, it helps to first understand the lifecycle of large language models (LLMs). Most LLMs begin with a stage called <strong>pre-training</strong>, where the model is trained from scratch—starting with random weights—on massive datasets that include internet text, books, Wikipedia, and other publicly available sources. This stage is incredibly compute-intensive, often requiring billions of tokens and weeks or months of training time. The goal of pre-training is to help the model learn general language patterns, reasoning skills, and world knowledge—but not to specialize in any particular task.</p>
<p>Once pre-training is complete, the result is a general-purpose model (often called a <strong>base model</strong> or <strong>foundation model</strong>) that understands language broadly but lacks domain-specific expertise or task-level optimization. This is where fine-tuning comes in.</p>
<p><strong>Fine-tuning</strong> is the process of taking this pre-trained model and training it further on a much smaller, specialized dataset tailored to a particular use case. This could include customer support transcripts, legal documents, or medical notes—whatever matches the target application. Instead of learning from scratch, the model starts with a strong foundation and simply adjusts its weights to better perform in your desired domain or task.</p>
<p>Compared to pre-training, fine-tuning is faster, more affordable, and highly targeted. It allows organizations to unlock the power of large models without the immense cost of building them from the ground up.</p>
<div class="cell" data-eval="true" data-layout-align="default">
<div class="cell-output-display">
<div>
<p></p><figure class="figure"><p></p>
<div>
<pre class="mermaid mermaid-js">flowchart LR
    A(["Raw Text Data 
    (Web, Books, Wikipedia)"]) -.-&gt; B{{Pre-training}}
    B --&gt; C((Base LLM))
    D(["Domain-Specific Dataset"]) -.-&gt; E{{Fine-tuning}}
    C --&gt; E
    E --&gt; F((Fine-Tuned LLM))

    %% Style assignments
    class A,D Sky;
    class B,E Ash;
    class C,F Aqua;

    %% Style definitions
    classDef Sky stroke-width:1px, stroke:#374D7C, fill:#E2EBFF, color:#374D7C;
    classDef Ash stroke-width:1px, stroke:#999999, fill:#EEEEEE, color:#000000;
    classDef Aqua stroke-width:1px, stroke:#46EDC8, fill:#DEFFF8, color:#378E7A;
</pre>
</div>
<p></p></figure><p></p>
</div>
</div>
</div>
<p>A useful analogy is training a college graduate for a new job. Pre-training is like sending someone through years of general education: they learn how to think, write, and analyze problems across many subjects. They graduate with broad knowledge but no experience in your specific company or domain.</p>
<p>Fine-tuning is like giving that graduate a few weeks of onboarding and role-specific training. You teach them your tools, your customers, your terminology. You don’t need to re-teach the fundamentals—they already have them. You’re simply refining their knowledge so they can do your job well.</p>
<hr>
<p><strong>Check Your Understanding</strong></p>
<div class="callout callout-style-default callout-tip callout-titled" title="What does it mean to train?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What does it mean to train?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>To train a language model means to adjust the internal parameters (called weights) of a neural network so it improves at predicting or generating language.</p>
<p>Instead of just <strong>telling</strong> the model what to do with a prompt, training actually <strong>shows</strong> the model what to do—by providing many examples of input and output pairs. Training bakes those patterns into the model itself. After training, the model doesn’t just follow instructions temporarily; it has learned new behavior permanently.</p>
<p>This is typically done using <strong>gradient descent</strong>, a process that compares the model’s prediction to the correct output, calculates the <strong>loss</strong> (error), and then updates the weights to reduce that loss in future predictions.</p>
<blockquote class="blockquote">
<p><strong>Example:</strong><br>
If the model sees the prompt *“The capital of France is ___“* and predicts <em>“Berlin”</em>, the loss will be high. The model then adjusts its internal weights to make <em>“Paris”</em> more likely next time.</p>
</blockquote>
<p>We will discuss training loss further in a later section when we prepare to fine-tune our own model.</p>
</div>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled" title="What are the key differences between pretraining and fine-tuning?">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What are the key differences between pretraining and fine-tuning?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 14%">
<col style="width: 40%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th>Aspect</th>
<th>Pretraining</th>
<th>Fine-Tuning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Starting Point</strong></td>
<td>From scratch (random weights)</td>
<td>From a pretrained model (existing weights)</td>
</tr>
<tr class="even">
<td><strong>Dataset Size</strong></td>
<td>Massive (web-scale data)</td>
<td>Small and domain/task-specific</td>
</tr>
<tr class="odd">
<td><strong>Objective</strong></td>
<td>Learn general language/world patterns</td>
<td>Specialize for a narrow use case</td>
</tr>
<tr class="even">
<td><strong>Compute Cost</strong></td>
<td>Extremely high (weeks of GPU time)</td>
<td>Relatively low (hours or days)</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p><strong>Remember:</strong> Pretraining is about learning <em>language</em>. Fine-tuning is about learning <em>your task</em>.</p>
</blockquote>
</div>
</div>
</div>
</section>
<section id="benefits-and-uses-of-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="benefits-and-uses-of-fine-tuning">Benefits and Uses of Fine-Tuning</h3>
<p>Fine-tuning is a powerful tool for making large language models more useful, focused, and efficient. It allows organizations to adapt general-purpose models to their specific needs, tasks, and data. Below are four of the most important reasons organizations choose to fine-tune their models:</p>
<p><strong>1. Cost-Effective and Compute-Efficient</strong></p>
<p>One of the primary benefits of fine-tuning is the significant reduction in both <strong>computational resources</strong> and <strong>ongoing API costs</strong>. Training a model from scratch requires vast amounts of data and compute, which can be prohibitively expensive. But even using hosted APIs from providers like OpenAI or Anthropic can become expensive at scale—especially if your application makes frequent or complex calls.</p>
<p>Fine-tuning an <strong>open-source model</strong> gives you long-term cost control by allowing you to host the model yourself and avoid usage-based API billing. By starting with a pretrained model—which has already learned general patterns and language structure—you can adapt it to your specific needs using relatively little data and compute.</p>
<p>This is especially valuable for startups, research teams, or smaller organizations looking to deploy models efficiently without relying on expensive external infrastructure.</p>
<p><strong>2. Domain Adaptation and Task Specialization</strong></p>
<p>Pretrained models are generalists: they understand language broadly, but they’re not experts in your specific domain. Fine-tuning allows you to specialize a model for a particular task (e.g., summarization, classification) or domain (e.g., legal, medical, education).</p>
<p>By training on examples from your target use case, the model learns domain-specific terminology, writing styles, and reasoning patterns. For example, a model fine-tuned on customer reviews can learn to detect nuanced sentiment more effectively than a generic model.</p>
<p>In many cases, a <strong>small fine-tuned model can outperform a much larger generalist LLM</strong> on a specific task. This results in faster inference, lower latency, and more relevant responses—even with far fewer parameters.</p>
<p><strong>3. Control Over Behavior</strong></p>
<p>Prompt engineering and retrieval techniques can guide what a model talks about, but they don’t fundamentally change how it reasons, responds, or formats answers.</p>
<p>Fine-tuning gives you much greater control over:</p>
<ul>
<li>The <strong>tone and style</strong> of responses (e.g., friendly, formal, concise)<br>
</li>
<li>The model’s <strong>workflow and logic</strong>, such as following step-by-step reasoning<br>
</li>
<li>The <strong>consistency</strong> of outputs across prompts and users</li>
</ul>
<p>This is how companies train AI to stay on-brand, follow safety guidelines, or mirror specific writing styles.</p>
<p><strong>4. Privacy and Security</strong></p>
<p>When working with sensitive or proprietary data, fine-tuning provides a way to embed that knowledge into the model without exposing it to external APIs.</p>
<p>Key benefits include:</p>
<ul>
<li><strong>Data remains in-house</strong> — fine-tuning can be done on private infrastructure<br>
</li>
<li><strong>No need to send sensitive context repeatedly</strong> via prompts<br>
</li>
<li><strong>Enables informed generation</strong> using non-public or confidential information</li>
</ul>
<p>This is critical for applications in healthcare, finance, government, and enterprise, where data privacy and compliance are essential.</p>
</section>
<section id="challenges-and-limitations-of-fine-tuning" class="level3">
<h3 class="anchored" data-anchor-id="challenges-and-limitations-of-fine-tuning">Challenges and Limitations of Fine-Tuning</h3>
<p>While fine-tuning can unlock powerful performance gains and customizations, it also comes with tradeoffs. Before deciding to fine-tune, teams should consider the limitations, risks, and operational challenges involved:</p>
<p><strong>1. Requires Technical Expertise and Infrastructure</strong></p>
<p>Fine-tuning is not as plug-and-play as prompt engineering or API-based solutions. It requires:</p>
<ul>
<li>Knowledge of machine learning workflows and frameworks (e.g., Hugging Face, LLaMA Factory)<br>
</li>
<li>Access to GPUs or specialized infrastructure<br>
</li>
<li>Ability to preprocess data, manage training loops, and troubleshoot errors</li>
</ul>
<p>This complexity can be a barrier for teams without dedicated ML or MLOps support.</p>
<p><strong>2. Maintenance and Drift</strong></p>
<p>Once you fine-tune a model, <strong>you own the lifecycle</strong>—including:</p>
<ul>
<li>Monitoring for performance drift as real-world data changes<br>
</li>
<li>Updating the model with new examples or edge cases<br>
</li>
<li>Managing versioning, rollback, and deployment</li>
</ul>
<p>This adds long-term operational overhead compared to hosted APIs that automatically improve over time.</p>
<p><strong>3. Data Collection and Labeling Cost</strong></p>
<p>High-quality fine-tuning depends on <strong>well-labeled, task-specific data</strong>. But collecting this data can be:</p>
<ul>
<li>Expensive (e.g., hiring annotators)<br>
</li>
<li>Time-consuming (especially for edge cases or nuanced outputs)<br>
</li>
<li>Inconsistent (if labeling guidelines aren’t followed strictly)</li>
</ul>
<p>Small datasets can still be effective, but their quality matters more than their size.</p>
<p><strong>4. Limited Transferability</strong></p>
<p>A model fine-tuned for one domain or task may not generalize well to others. Unlike prompt-based approaches that can reuse the same model across many tasks, fine-tuned models tend to be <strong>narrowly specialized</strong>.</p>
<p>This can require maintaining multiple models or fine-tuning different variants for each use case, adding complexity to your AI stack.</p>
<hr>
<p><strong>In summary</strong>, fine-tuning is a critical technique for turning general-purpose language models into strategic AI solutions. It allows organizations to adapt pretrained models to their specific domains, tasks, and privacy constraints—enabling cost savings, improved performance, brand consistency, and secure handling of proprietary data. This makes fine-tuning especially valuable for teams that need deeper customization beyond what prompt engineering or retrieval-based methods can provide. However, it comes with meaningful tradeoffs: fine-tuning requires technical expertise, high-quality labeled data, and long-term model maintenance. When used thoughtfully, fine-tuning offers powerful returns—but should be weighed against its complexity and cost relative to lighter-weight alternatives.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Comparison: Prompt Engineering vs RAG vs Fine-Tuning
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<style>
  table.clean-table th,
  table.clean-table td {
    padding: 6px 10px;
    text-align: center;
  }
  table.clean-table th:first-child,
  table.clean-table td:first-child {
    text-align: left;
  }
</style>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 21%">
</colgroup>
<thead>
<tr class="header">
<th>Criteria</th>
<th>Prompt Engineering</th>
<th>RAG</th>
<th>Fine-Tuning</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Ease of Deployment</td>
<td>High</td>
<td>Medium</td>
<td>Low</td>
</tr>
<tr class="even">
<td>Domain Adaptation</td>
<td>Low</td>
<td>High</td>
<td>High</td>
</tr>
<tr class="odd">
<td>Factual Accuracy</td>
<td>Low</td>
<td>High</td>
<td>Moderate (static)</td>
</tr>
<tr class="even">
<td>Control over Output</td>
<td>Limited</td>
<td>Moderate</td>
<td>High</td>
</tr>
<tr class="odd">
<td>Privacy-Friendly</td>
<td>High</td>
<td>Moderate (depends)</td>
<td>High</td>
</tr>
<tr class="even">
<td>Supports Dynamic Content</td>
<td>Low</td>
<td>High</td>
<td>Low</td>
</tr>
<tr class="odd">
<td>Low Latency / Offline Use</td>
<td>Low</td>
<td>Moderate</td>
<td>High</td>
</tr>
<tr class="even">
<td>Consistency / Repeated Tasks</td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
<tr class="odd">
<td>Upfront Effort</td>
<td>Low</td>
<td>Medium</td>
<td>High</td>
</tr>
</tbody>
</table>
<blockquote class="blockquote">
<p>Use this table to compare the tradeoffs between Prompt Engineering, Retrieval-Augmented Generation (RAG), and Fine-Tuning depending on your use case.</p>
</blockquote>
</div>
</div>
</div>
</section>
<section id="how-fine-tuning-works-a-roadmap" class="level3">
<h3 class="anchored" data-anchor-id="how-fine-tuning-works-a-roadmap">How Fine-Tuning Works: A Roadmap</h3>
<p>In order to fine-tune our own language model, we will walk through the following steps:</p>
<ol type="1">
<li><p><strong>Choose a Base Model</strong><br>
Select a pre-trained model that aligns with your task needs (e.g.&nbsp;LLaMA, Mistral, Falcon).</p></li>
<li><p><strong>Prepare a Specialized Dataset</strong><br>
Gather and format examples specific to your domain or task. The quality of this data will directly shape the model’s behavior.</p></li>
<li><p><strong>Choose a Fine-Tuning Approach: Technique + Framework</strong> Decide whether to fully fine-tune the model or use a parameter-efficient method like LoRA, QLoRA, or adapters.</p></li>
<li><p><strong>Set Training Arguments (Hyperparameters)</strong><br>
Choose your learning rate, batch size, number of epochs, and other settings that influence how the model learns.</p></li>
<li><p><strong>Train, Iterate, and Evaluate</strong><br>
Begin training, monitor the loss, validate the model, and adjust as needed. Use benchmarks and real-world testing to assess performance.</p></li>
</ol>
<blockquote class="blockquote">
<p>We’ll walk through each of these steps in the sections that follow.</p>
</blockquote>
</section>
</section>
<section id="choose-a-base-model" class="level2">
<h2 class="anchored" data-anchor-id="choose-a-base-model">1) Choose a Base Model</h2>
<p>The first decision in any fine-tuning project is selecting a base model—the pre-trained large language model (LLM) you will adapt for your task. Your choice here impacts everything downstream: performance, cost, deployment complexity, techniques/frameworks, and even data formatting.</p>
<p>There is no one-size-fits-all answer. Here are the main criteria to guide your decision:</p>
<section id="model-size-number-of-parameters" class="level3">
<h3 class="anchored" data-anchor-id="model-size-number-of-parameters"><strong>Model Size (Number of Parameters)</strong></h3>
<p>One of the first and most fundamental choices when selecting a base model is its size—typically measured by the number of parameters it contains.</p>
<p><strong>Larger models</strong> (typically &gt;7B parameters) can capture more complex language patterns, reason more deeply, and perform better across a wide range of tasks. However, this power comes at a cost: they require significantly more compute, memory, and training time. They are best suited for high-stakes tasks or enterprise-grade applications with strong infrastructure.</p>
<p><strong>Smaller models</strong> (typically 1–3B parameters) are much easier to fine-tune on consumer-grade hardware. They train faster, cost less to deploy, and are ideal for lightweight or domain-specific use cases—like customer service chatbots, internal tools, or embedded AI assistants.</p>
<blockquote class="blockquote">
<p><strong>Rule of thumb:</strong><br>
Choose the smallest model that can still meet your task’s performance needs. For many real-world applications—especially in narrow domains—small and medium models can be surprisingly capable when fine-tuned correctly.</p>
</blockquote>
<table class="caption-top table">
<colgroup>
<col style="width: 18%">
<col style="width: 21%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th>Model Size</th>
<th>Parameter Range</th>
<th>Use Case Fit</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Small</td>
<td>~1B–3B</td>
<td>Fast prototyping, edge devices, low-latency apps</td>
</tr>
<tr class="even">
<td>Medium</td>
<td>~3B–7B</td>
<td>Balanced performance vs.&nbsp;compute cost</td>
</tr>
<tr class="odd">
<td>Large</td>
<td>&gt;7B</td>
<td>High accuracy, broader generalization</td>
</tr>
</tbody>
</table>
</section>
<section id="pre-trained-vs-instruction-tuned" class="level3">
<h3 class="anchored" data-anchor-id="pre-trained-vs-instruction-tuned"><strong>Pre-Trained vs Instruction Tuned</strong></h3>
<p>The process of fine-tuning often happens in stages. You don’t always need to start from a raw, base model. Some models have already gone through a form of fine-tuning specifically designed to help them follow human instructions—this is called <strong>instruction tuning</strong>. The result is an <strong>instruction-tuned model</strong>, which is better suited for tasks like responding to prompts in a chatbot or assistant-like manner.</p>
<p>Instruction-tuned models are typically the best choice when building for real-world use cases like chat, summarization, or customer support. Because they already understand how to follow instructions, they require less effort and data to fine-tune effectively. Base (pretrained) models, on the other hand, offer more flexibility and control, but require more work—such as formatting data carefully, defining tasks explicitly, and teaching the model how to respond appropriately from scratch.</p>
</section>
<section id="licensing-and-commercial-use" class="level3">
<h3 class="anchored" data-anchor-id="licensing-and-commercial-use"><strong>Licensing and Commercial Use</strong></h3>
<p>One of the most overlooked—but critical—factors when selecting a base model is its <strong>license</strong>. Not all open-source models are truly open for commercial use.</p>
<p>Some models, like <strong>Meta’s LLaMA 2</strong>, are released under <strong>non-commercial research licenses</strong>. This means you can experiment, research, and even fine-tune them—but you cannot deploy them in a commercial product without explicit permission. In contrast, models like <strong>Mistral</strong>, <strong>Qwen</strong>, or <strong>Falcon</strong> often use more permissive licenses (e.g., <strong>Apache 2.0</strong>), allowing full commercial use, redistribution, and modification.</p>
<p>Licensing directly impacts your go-to-market options:</p>
<ul>
<li>If you’re prototyping an internal tool or doing academic research, a research-only license may be acceptable.<br>
</li>
<li>But if you’re building a product, deploying to customers, or embedding the model in a commercial offering, using a model with commercial restrictions could put you at <strong>legal risk</strong>.</li>
</ul>
<blockquote class="blockquote">
<p><strong>Always check the model’s license</strong> (typically found on Hugging Face or GitHub) before beginning fine-tuning. Licensing restrictions often apply not just to the base model but also to any fine-tuned derivatives.</p>
</blockquote>
</section>
<section id="model-compatibility-and-alignment" class="level3">
<h3 class="anchored" data-anchor-id="model-compatibility-and-alignment"><strong>Model Compatibility and Alignment</strong></h3>
<p>Choosing a base model isn’t just about performance — it determines how you’ll train, fine-tune, and work with the model going forward. Each model family (like LLaMA or Mistral) comes with its own expectations and requirements, which will shape every step of your fine-tuning pipeline.</p>
<blockquote class="blockquote">
<p>Choosing a model is like choosing a device with a specific charging port. Once you commit, all your cables, adapters, and accessories need to match. Some models work seamlessly with common tools—others may need extra setup or won’t be compatible at all</p>
</blockquote>
<p>Here are four key things your model choice affects. These topics will be covered more in-depth later in this chapter:</p>
<p><strong>1. Tokenizer</strong><br>
Every model processes text using a tokenizer — a way of breaking sentences into chunks (called <em>tokens</em>) that the model understands. Each model family has its own tokenizer, and they aren’t interchangeable. Once you pick a model, you’re locked into its tokenizer for both training and inference.</p>
<p><strong>2. Data Format</strong><br>
Different models expect training data (input and output pairs) to follow specific formats. If your data format doesn’t match what the model expects, it won’t learn effectively.</p>
<p><strong>3. Frameworks</strong><br>
Frameworks are the tools you use to fine-tune a model. Examples include <strong>LLaMA Factory</strong>, <strong>Hugging Face Trainer</strong>, and <strong>Axolotl</strong>. Each framework only supports certain model types, file structures, and features. If your chosen model isn’t supported by your preferred framework, you’ll either have to switch tools or make significant customizations.</p>
<p><strong>4. Techniques</strong><br>
Fine-tuning techniques—like <strong>LoRA</strong>, <strong>QLoRA</strong>, and <strong>full fine-tuning</strong>—vary in how they update the model’s weights and how much compute they require. Some models are optimized for lightweight, efficient techniques like LoRA; others may require full fine-tuning to see meaningful improvements. Your model choice can limit or enable which approaches are available.</p>
<p>Together, these form a kind of alignment: the better your tools, data, and training method match the structure and expectations of the model, the smoother your fine-tuning process will be—and the better your results.</p>
<p>In the next sections, we’ll walk through how to prepare aligned data for your model and choose the training method that best fits your use case and enhances training speed and efficiency.</p>
</section>
</section>
<section id="data-preparation-for-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation-for-fine-tuning">2) Data Preparation for Fine-Tuning</h2>
<p>Fine-tuning is fundamentally a form of supervised learning — teaching a model by showing it many examples of inputs paired with ideal outputs so it learns to replicate those patterns. This section walks through the three core stages of data preparation: <strong>sourcing</strong>, <strong>cleaning</strong>, and <strong>formatting</strong>.</p>
<div class="callout callout-style-default callout-note callout-titled" title="Supervised vs. Self-Supervised Learning">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Supervised vs.&nbsp;Self-Supervised Learning
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Pre-training uses <strong>self-supervised learning</strong>, where the model learns from raw text by predicting missing parts—no labeled answers are provided.</p>
<p>Fine-tuning uses <strong>supervised learning</strong>, where each example has a clearly defined input and target output. This means your data must be formatted with both parts: what the model should see, and what it should generate.</p>
<p>The structure of your dataset—input → output—is what makes fine-tuning supervised.</p>
</div>
</div>
</div>
<section id="data-sourcing" class="level3">
<h3 class="anchored" data-anchor-id="data-sourcing"><strong>Data Sourcing</strong></h3>
<p>The first step is identifying where your task-specific data will come from. Common sources include:</p>
<ul>
<li><strong>Internal logs</strong> – e.g., customer support transcripts, chatbot conversations, or form submissions<br>
</li>
<li><strong>Public datasets</strong> – from platforms like <a href="https://huggingface.co/datasets?sort=trending">Hugging Face Datasets</a>, Kaggle, or academic benchmarks<br>
</li>
<li><strong>Manual generation</strong> – examples written or annotated by domain experts<br>
</li>
<li><strong>Synthetic generation</strong> – data generated by an LLM and later reviewed or edited for quality</li>
</ul>
<p>Fine-tuning doesn’t require massive datasets. What matters is that each example is correct, relevant, and representative. A few hundred high-quality examples often outperform thousands of noisy ones.</p>
<p>The most important thing is that the data reflects the <strong>task and tone</strong> you want the model to learn. If you’re building a customer support assistant, use examples from real interactions. If you’re building a tutor, use educational prompts and responses.</p>
</section>
<section id="data-cleaning" class="level3">
<h3 class="anchored" data-anchor-id="data-cleaning"><strong>Data Cleaning</strong></h3>
<p>Raw examples often need cleaning before they’re ready for training. This step focuses on eliminating noise and inconsistencies. Common cleaning tasks include:</p>
<ul>
<li>Removing incomplete or corrupted entries<br>
</li>
<li>Fixing typos, inconsistent punctuation, or formatting issues<br>
</li>
<li>Standardizing casing, spacing, or syntax<br>
</li>
<li>Removing sensitive or personally identifiable information (PII)</li>
</ul>
<p>Clean data helps ensure the model learns meaningful patterns—not accidental ones.</p>
</section>
<section id="data-formatting" class="level3">
<h3 class="anchored" data-anchor-id="data-formatting"><strong>Data Formatting</strong></h3>
<p>Regardless of the task, all fine-tuning datasets must include a clear prompt (or context) and a desired response. However, the <strong>format</strong> of this information depends on the model architecture and training history.</p>
<p>Below are the most common and important formats in modern fine-tuning workflows—each suited to different use cases.</p>
<p>Data is usually stored in <strong>JSON</strong> or <strong>JSON Lines (JSONL)</strong> formats.</p>
<section id="common-data-formats" class="level4">
<h4 class="anchored" data-anchor-id="common-data-formats">Common Data Formats</h4>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-5-contents" aria-controls="callout-5" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Instruction Format (<code>instruction → input → output</code>)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-5" class="callout-5-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Best for:</strong> Task-specific fine-tuning like summarization, translation, classification, and question answering <strong>Model types</strong>: FLAN-T5, Alpaca, LLaMA Factory-compatible models</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"instruction"</span><span class="fu">:</span> <span class="st">"Translate this sentence to Spanish."</span><span class="fu">,</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"input"</span><span class="fu">:</span> <span class="st">"How are you?"</span><span class="fu">,</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"output"</span><span class="fu">:</span> <span class="st">"¿Cómo estás?"</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-6-contents" aria-controls="callout-6" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Chat Format (<code>messages</code> list)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-6" class="callout-6-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>This format simulates a conversation between a user and an assistant over multiple dialogue turns. Useful for fine-tuning chat-style models that require conversational memory and role-awareness.</p>
<p><strong>Best for</strong>: Best for: Chatbots, AI assistants, and multi-turn conversational agents <strong>Model types</strong>: LLaMA-2-Chat, Mistral-Instruct, OpenChat, ShareGPT-style models</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"conversations"</span><span class="fu">:</span> <span class="ot">[</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span> <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"user"</span><span class="fu">,</span> <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"How do I reset my password?"</span> <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span> <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"assistant"</span><span class="fu">,</span> <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"Go to Settings &gt; Account &gt; Reset Password and follow the instructions."</span> <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span> <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"user"</span><span class="fu">,</span> <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"Thanks!"</span> <span class="fu">}</span><span class="ot">,</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">{</span> <span class="dt">"role"</span><span class="fu">:</span> <span class="st">"assistant"</span><span class="fu">,</span> <span class="dt">"content"</span><span class="fu">:</span> <span class="st">"You're welcome!"</span> <span class="fu">}</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  <span class="ot">]</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-7-contents" aria-controls="callout-7" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Prompt-Completion Format (<code>prompt → response</code>)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-7" class="callout-7-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Best for</strong>: Simple, single-turn generation tasks like content generation, Q&amp;A, or creative writing<br>
<strong>Model types</strong>: GPT-2, Mistral (base), Falcon, and other decoder-only models</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"Write a tagline for a fitness app:"</span><span class="fu">,</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"response"</span><span class="fu">:</span> <span class="st">"Train smart. Live strong."</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-8-contents" aria-controls="callout-8" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Classification Format (<code>text → label</code>)
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-8" class="callout-8-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p><strong>Best for</strong>: Sentiment analysis, topic detection, intent classification, and other labeling tasks<br>
<strong>Model types</strong>: BERT-style models, RoBERTa, and decoder models fine-tuned for classification</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode json code-with-copy"><code class="sourceCode json"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"text"</span><span class="fu">:</span> <span class="st">"The interface was slow and hard to use."</span><span class="fu">,</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">"label"</span><span class="fu">:</span> <span class="st">"negative"</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<hr>
</section>
<section id="summary" class="level4">
<h4 class="anchored" data-anchor-id="summary"><strong>Summary</strong></h4>
<p>The format of your dataset is just as important as its content. A model can only learn effectively if examples are structured in a way it understands. Fine-tuning isn’t just about feeding the model data—it’s about providing clear, consistent demonstrations of the behavior you want it to learn. Whether you’re training a chatbot, an instruction-follower, or a classifier, you need to choose a format that aligns with your model’s architecture and training history. Clean formatting ensures the model can focus on learning patterns—instead of being confused by structure.</p>
</section>
</section>
</section>
<section id="techniques-and-tools-for-fine-tuning" class="level2">
<h2 class="anchored" data-anchor-id="techniques-and-tools-for-fine-tuning">3) Techniques and Tools for Fine-Tuning</h2>
<p>Once you’ve chosen your base model and prepared your dataset, the next step is deciding <strong>how</strong> you’ll fine-tune it. This involves two key decisions:</p>
<ul>
<li><strong>Technique</strong> — <em>What kind of fine-tuning method will you use?</em> (e.g., full fine-tuning, LoRA)</li>
<li><strong>Tools (Frameworks)</strong> — <em>Which tool or codebase will you use to apply that method?</em> (e.g., LLaMA Factory, Hugging Face)</li>
</ul>
<p>Both choices depend on your goals, model size, and available compute.</p>
<section id="fine-tuning-techniques" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-techniques"><strong>Fine-Tuning Techniques</strong></h3>
<p>Fine-tuning techniques define <strong>how much of the model gets updated</strong> during training—and how efficiently that update process can be done.</p>
<p>There are many emerging strategies in this space, and the field continues to evolve rapidly. For the scope of this course, we’ll focus primarily on <strong>full parameter fine-tuning</strong> and <strong>PEFT methods (LoRA and QLoRA)</strong>.</p>
<section id="full-parameter-fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="full-parameter-fine-tuning"><strong>Full Parameter Fine-Tuning</strong></h4>
<p>Full parameter fine-tuning involves updating <strong>all of the weights</strong> in the model during training. This gives you maximum control and flexibility—you can adapt the model deeply to a new domain or task.</p>
<p>However, it comes at a cost: full fine-tuning is <strong>compute-intensive</strong>, <strong>memory-heavy</strong>, and often requires <strong>high-end GPUs</strong>. It also risks overfitting if your dataset is small.</p>
<p>Because of these tradeoffs, full fine-tuning is typically used when:</p>
<ul>
<li>You have access to strong infrastructure (e.g., multi-GPU or cloud clusters)</li>
<li>You’re working with a small model (e.g., under 1B parameters)</li>
<li>You need to significantly change the model’s behavior</li>
</ul>
<p>For most use cases, <strong>PEFT methods</strong> offer similar performance with far fewer resources.</p>
</section>
<section id="parameter-efficient-fine-tuning-peft" class="level4">
<h4 class="anchored" data-anchor-id="parameter-efficient-fine-tuning-peft"><strong>Parameter Efficient Fine-Tuning (PEFT)</strong></h4>
<p>PEFT is an umbrella term for many techniques allow you to fine-tune large models by updating <strong>only a small number of parameters</strong>, rather than the entire model. This makes training much faster, less memory-intensive, and possible on consumer-grade hardware.</p>
<p>Instead of modifying all the weights, PEFT methods freeze the original model and train <strong>small, added components</strong>—like adapter layers or low-rank matrices—that learn task-specific behavior.</p>
<p>In this course, we’ll focus on one of the most widely used PEFT technique: <strong>LoRA (Low-Rank Adaptation)</strong>, which uses lightweight matrix updates to adapt the model efficiently while keeping the original weights frozen.</p>
<div id="fig-peft-methods" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-peft-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="images/peft-methods.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:80.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-peft-methods-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8.1: Overview of PEFT Methods
</figcaption>
</figure>
</div>
<hr>
<section id="low-rank-adaptation-lora" class="level5">
<h5 class="anchored" data-anchor-id="low-rank-adaptation-lora"><strong>Low Rank Adaptation (LoRA)</strong></h5>
<p>LoRA (Low-Rank Adaptation) is a clever way to fine-tune large language models without changing most of their weights.</p>
<p>Instead of updating all the millions (or even billions!) of parameters in a model, LoRA adds a few small, trainable layers that sit inside the existing model architecture. During training, only these small layers are updated, while the original model remains frozen. This makes training faster, cheaper, and requires much less memory.</p>
<p>Think of it like adding a few adjustable knobs to an already well-built machine, instead of rebuilding the entire machine from scratch.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/lora-regular.png" class="img-fluid figure-img"></p>
<figcaption>Comparison of full parameter fine-tuning vs LoRA</figcaption>
</figure>
</div>
</section>
<section id="quantized-low-rank-adaptation-qlora" class="level5">
<h5 class="anchored" data-anchor-id="quantized-low-rank-adaptation-qlora"><strong>Quantized Low Rank Adaptation (QLoRA)</strong></h5>
<p>While LoRA dramatically reduces the number of parameters you need to train, the base model (like LLaMA or Mistral) still takes up a lot of memory. That’s where QLoRA (Quantized LoRA) comes in.</p>
<p>QLoRA keeps everything that makes LoRA efficient—but adds quantization to reduce the memory footprint of the base model itself. This makes it possible to fine-tune large models even on laptops or free Colab GPUs.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-9-contents" aria-controls="callout-9" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
What is Quantization?
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-9" class="callout-9-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<p>Quantization is a way of making a model smaller by storing its weights using <strong>fewer bits</strong>.</p>
<p>Most models use <strong>16-bit or 32-bit floats</strong> to store numbers.<br>
<strong>QLoRA</strong> uses <strong>4-bit integers</strong>, which are much smaller.</p>
<p>This doesn’t change the model’s structure—it just changes how the numbers are stored in memory.</p>
<p>It’s like switching from a high-resolution video to a compressed version that still looks good—but takes up less space.</p>
</div>
</div>
</div>
<p>For a deeper understanding of LoRA and QLoRA, <a href="https://www.youtube.com/watch?v=t1caDsMzWBk">watch this video</a>.</p>
</section>
</section>
</section>
<section id="fine-tuning-tools-no-code-to-low-code" class="level3">
<h3 class="anchored" data-anchor-id="fine-tuning-tools-no-code-to-low-code"><strong>Fine-Tuning Tools (No-Code to Low-Code)</strong></h3>
<p>If you’re just getting started or want to fine-tune without writing a lot of code, the following tools offer simple, powerful interfaces:</p>
<section id="openai-platform" class="level4">
<h4 class="anchored" data-anchor-id="openai-platform"><strong><a href="https://platform.openai.com/" target="_blank">OpenAI Platform</a></strong></h4>
<p>A hosted solution for fine-tuning OpenAI models like <code>gpt-3.5-turbo</code>. You upload your training data, and OpenAI handles the rest. No infrastructure or model setup needed.</p>
<p><strong>Pros</strong>: Easy to use, fully managed, excellent for production-ready APIs<br>
<strong>Cons</strong>: Only supports OpenAI models (not open-source), limited customization</p>
</section>
<section id="hugging-face-autotrain" class="level4">
<h4 class="anchored" data-anchor-id="hugging-face-autotrain"><strong><a href="https://huggingface.co/autotrain" target="_blank">Hugging Face AutoTrain</a></strong></h4>
<p>A no-code/low-code tool for training and fine-tuning Hugging Face models using your own data. Supports classification, summarization, instruction tuning, and more.</p>
<p><strong>Pros</strong>: Very user-friendly UI, supports both public and private models<br>
<strong>Cons</strong>: Less control than coding directly with the Transformers library</p>
</section>
<section id="llama-factory" class="level4">
<h4 class="anchored" data-anchor-id="llama-factory"><strong><a href="https://github.com/hiyouga/LLaMA-Factory" target="_blank">LLaMA-Factory</a></strong></h4>
<p>A lightweight framework (requires Python) for fine-tuning open-source models like LLaMA, Mistral, and Qwen. Supports LoRA, QLoRA, and other efficient training methods.</p>
<p><strong>Pros</strong>: High flexibility, supports many models and fine-tuning strategies<br>
<strong>Cons</strong>: Requires setup and command-line use</p>
</section>
</section>
</section>
<section id="training-arguments-and-hyperparameter-optimization" class="level2">
<h2 class="anchored" data-anchor-id="training-arguments-and-hyperparameter-optimization">4) Training Arguments and Hyperparameter Optimization</h2>
<p>When fine-tuning a language model, you need to configure how the training process will run. These configurations are called hyperparameters. Hyperparameters are like <strong>settings or dials</strong> that you tune before training begins. They define how the model learns, how fast it learns, how long it trains, and how much data it sees at a time. Choosing the right hyperparameters is critical to getting good result. If they’re set poorly, the model may underfit (learn too little) or overfit (memorize the data instead of generalizing). Good choices help the model learn effectively and perform well on new data.</p>
<p>Below are the most important hyperparameters you’ll encounter when fine-tuning a language model. For each, we define what it is, how it affects the training process, and provide common values.</p>
<section id="learning-rate" class="level3">
<h3 class="anchored" data-anchor-id="learning-rate"><strong>1) Learning Rate</strong></h3>
<p><strong>Definition</strong>:<br>
Controls how much the model’s weights are adjusted with each training step.</p>
<p><strong>Effect on Training</strong>:<br>
- Too high → unstable training, poor convergence<br>
- Too low → slow learning or getting stuck in suboptimal solutions</p>
<p><strong>Common Values</strong>:<br>
<code>1e-5</code>, <code>2e-5</code>, <code>5e-5</code>, <code>1e-4</code>, <code>2e-4</code><br>
(LoRA/QLoRA often use <code>1e-4</code> to <code>2e-4</code>)</p>
</section>
<section id="number-of-epochs" class="level3">
<h3 class="anchored" data-anchor-id="number-of-epochs"><strong>2) Number of Epochs</strong></h3>
<p><strong>Definition</strong>:<br>
The number of times the model iterates over the entire training dataset.</p>
<p><strong>Effect on Training</strong>:<br>
- More epochs → more learning opportunity<br>
- Too many → overfitting on small datasets</p>
<p><strong>Common Values</strong>:<br>
<code>1</code>, <code>2</code>, <code>3</code> (up to <code>10</code> for small datasets)</p>
</section>
<section id="batch-size" class="level3">
<h3 class="anchored" data-anchor-id="batch-size"><strong>3) Batch Size</strong></h3>
<p><strong>Definition</strong>:<br>
Number of training examples processed simultaneously per GPU.</p>
<p><strong>Effect on Training</strong>:<br>
- Larger batch size → faster training, more stable gradients<br>
- Smaller batch size → lower memory usage (can be paired with gradient accumulation)</p>
<p><strong>Common Values</strong>:<br>
<code>2</code>, <code>4</code>, <code>8</code>, <code>16</code><br>
(LoRA/QLoRA typically use <code>2</code> or <code>4</code>)</p>
</section>
<section id="gradient-accumulation-steps" class="level3">
<h3 class="anchored" data-anchor-id="gradient-accumulation-steps"><strong>4) Gradient Accumulation Steps</strong></h3>
<p><strong>Definition</strong>:<br>
Delays weight updates by accumulating gradients over multiple mini-batches.</p>
<p><strong>Effect on Training</strong>:<br>
- Simulates a larger effective batch size without needing more memory<br>
- Helps stabilize training with small per-device batch sizes</p>
<p><strong>Example</strong>:<br>
Batch size <code>2</code> + accumulation <code>4</code> = effective batch size <code>8</code></p>
<p><strong>Common Values</strong>:<br>
<code>1</code>, <code>2</code>, <code>4</code>, <code>8</code></p>
</section>
<section id="cutoff-length" class="level3">
<h3 class="anchored" data-anchor-id="cutoff-length"><strong>5) Cutoff Length</strong></h3>
<p><strong>Definition</strong>:<br>
Maximum number of tokens used from each input during training.</p>
<p><strong>Effect on Training</strong>:<br>
- Longer inputs = more context, but higher memory usage<br>
- Shorter inputs = lower memory usage, but possible information loss</p>
<p><strong>Common Values</strong>:<br>
<code>512</code>, <code>1024</code>, <code>2048</code></p>
</section>
<section id="validation-size" class="level3">
<h3 class="anchored" data-anchor-id="validation-size"><strong>6) Validation Size</strong></h3>
<p><strong>Definition</strong>:<br>
Percentage of data reserved for validation (not used in training).</p>
<p><strong>Effect on Training</strong>:<br>
- Enables tracking of model performance during training<br>
- Helps detect overfitting</p>
<p><strong>Common Values</strong>:<br>
<code>0.05</code>, <code>0.1</code>, <code>0.2</code><br>
(Set to <code>0</code> to disable validation)</p>
<p>Understanding these hyperparameters—and adjusting them thoughtfully—can dramatically improve the results of your fine-tuning runs.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-10-contents" aria-controls="callout-10" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary Table
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-10" class="callout-10-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 21%">
<col style="width: 35%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Hyperparameter</th>
<th>What It Controls</th>
<th>Why It Matters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>learning_rate</code></td>
<td>How quickly the model updates its weights</td>
<td>Too high: unstable training; too low: slow or no learning</td>
</tr>
<tr class="even">
<td><code>num_train_epochs</code></td>
<td>How many times the model sees the full dataset</td>
<td>More epochs = more learning, but also risk of overfitting</td>
</tr>
<tr class="odd">
<td><code>per_device_train_batch_size</code></td>
<td>How many examples the model processes at once</td>
<td>Larger batches are faster but use more memory</td>
</tr>
<tr class="even">
<td><code>gradient_accumulation_steps</code></td>
<td>Simulates larger batches over multiple steps</td>
<td>Useful when memory is limited</td>
</tr>
<tr class="odd">
<td><code>cutoff_len</code></td>
<td>Maximum number of tokens per example</td>
<td>Truncates long inputs; controls memory usage</td>
</tr>
<tr class="even">
<td><code>val_size</code></td>
<td>Portion of data used for validation (e.g., 0.1)</td>
<td>Helps track performance on unseen data</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p>In summary, hyperparameters are not learned during training—they’re chosen by you before training begins. The ideal values depend on several factors, including your model size, dataset, available hardware, and specific goals. Fortunately, you don’t have to guess: most tools offer solid default settings, which you can use as a starting point and adjust based on your results.</p>
</section>
</section>
<section id="evaluating-training-with-the-loss-curve" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-training-with-the-loss-curve">5) Evaluating Training with the Loss Curve</h2>
<p>When fine-tuning a model, one of the simplest and most useful diagnostic tools is the <strong>loss curve</strong>—a plot of the model’s training and validation loss across training steps. It gives you direct feedback on how well your model is learning and whether it’s likely to generalize to new data.</p>
<section id="understanding-loss" class="level3">
<h3 class="anchored" data-anchor-id="understanding-loss">Understanding Loss</h3>
<p>During training, your model tries to <strong>minimize the loss</strong>—a numerical estimate of how wrong its predictions are. A lower loss means better performance on the task at hand.</p>
<p>However, <strong>low training loss alone is not enough</strong>. What really matters is how well the model performs on unseen (validation) data. That’s where the idea of <strong>generalization</strong> comes in.</p>
<blockquote class="blockquote">
<p>A well-generalizing model will have both low training loss and low validation loss.</p>
</blockquote>
</section>
<section id="underfitting" class="level3">
<h3 class="anchored" data-anchor-id="underfitting">Underfitting</h3>
<p>Underfitting occurs when the model <strong>fails to learn the core patterns</strong> in the training data. This can happen if the model is too simple, trained for too few steps, or if the data lacks clear structure.</p>
<div style="float: right; margin-left: 20px; margin-bottom: 10px; text-align: center;">
<img src="images/underfitting.png" alt="Underfitting Loss Curve" width="280">
<p style="font-size: 0.85em; color: gray;">
Underfitting Loss Curve
</p>
</div>
<p>On a loss curve, underfitting shows up as:</p>
<ul>
<li><strong>High training loss</strong> that doesn’t improve much</li>
<li><strong>High validation loss</strong> that stays close to the training loss</li>
</ul>
<p>This means the model is struggling even on the examples it has seen.</p>
<p><strong>How to fix it:</strong> - Train longer (increase <code>num_train_epochs</code>) - Use a larger model or increase LoRA rank - Improve the quality and clarity of your dataset</p>
</section>
<section id="overfitting" class="level3">
<h3 class="anchored" data-anchor-id="overfitting">Overfitting</h3>
<p>Overfitting happens when the model learns <strong>too much</strong> from the training data—memorizing noise or quirks instead of learning generalizable patterns.</p>
<div style="float: right; margin-left: 20px; margin-bottom: 10px; text-align: center;">
<img src="images/overfitting.png" alt="Overfitting Loss Curve" width="280">
<p style="font-size: 0.85em; color: gray;">
Overfitting Loss Curve
</p>
</div>
<p>On the loss curve, this appears as:</p>
<ul>
<li><strong>Low training loss</strong></li>
<li><strong>Validation loss starts increasing</strong> after a certain point</li>
</ul>
<p>This growing gap is called the <strong>generalization gap</strong>—it shows the model is performing worse on unseen data, even as it continues to improve on training data.</p>
<p><strong>How to fix it:</strong></p>
<ul>
<li>Stop training earlier (reduce <code>num_train_epochs</code>)</li>
<li>Use regularization (e.g., dropout)</li>
<li>Use a smaller model or lower LoRA rank</li>
<li>Add more diverse or augmented data</li>
</ul>
</section>
<section id="a-healthy-fit-good-generalization" class="level3">
<h3 class="anchored" data-anchor-id="a-healthy-fit-good-generalization">A Healthy Fit (Good Generalization)</h3>
<p>A well-tuned model finds the sweet spot: it learns useful patterns from the training data <strong>without overfitting</strong>. This is the ideal scenario for most fine-tuning tasks.</p>
<div style="float: right; margin-left: 20px; margin-bottom: 10px; text-align: center;">
<img src="images/healthy-loss-curve.png" alt="Healthy Loss Curve" width="280">
<p style="font-size: 0.85em; color: gray;">
Healthy Loss Curve
</p>
</div>
<p>On the loss curve, you’ll see:</p>
<ul>
<li><strong>Training and validation loss both decrease steadily</strong></li>
<li>The two curves stay close together</li>
<li>Loss values flatten as training converges</li>
</ul>
<p>This means the model is generalizing well and ready for real-world usage.</p>
<p><strong>How to maintain a good fit:</strong></p>
<ul>
<li>Use early stopping</li>
<li>Monitor the validation loss closely</li>
<li>Regularly evaluate on real task outputs</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-11-contents" aria-controls="callout-11" aria-expanded="false" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Summary: How to Interpret the Loss Curve
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-11" class="callout-11-contents callout-collapse collapse">
<div class="callout-body-container callout-body">
<table class="caption-top table">
<colgroup>
<col style="width: 19%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th>Pattern</th>
<th>Training Loss</th>
<th>Validation Loss</th>
<th>Model Behavior</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Underfitting</strong></td>
<td>High</td>
<td>High</td>
<td>Not learning enough</td>
</tr>
<tr class="even">
<td><strong>Overfitting</strong></td>
<td>Low</td>
<td>Starts increasing</td>
<td>Memorizing training</td>
</tr>
<tr class="odd">
<td><strong>Good Fit</strong></td>
<td>Decreasing</td>
<td>Closely follows train</td>
<td>Generalizing well</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p><strong>Final Tip: Don’t Just Watch the Curve</strong></p>
<p>The loss curve is a powerful guide—but it’s not the whole story. After training, always evaluate your model’s outputs directly:</p>
<ul>
<li>Are they more accurate, helpful, or aligned?</li>
<li>Can you do side-by-side comparisons with the base model?</li>
<li>Does it solve the original task more effectively?</li>
</ul>
<p>You’ll learn more about advanced evaluation techniques in a future chapter.</p>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Fine-tuning is one of the most effective ways to adapt a pre-trained language model to your specific tasks, data, and goals. Unlike prompt engineering or retrieval-augmented generation, fine-tuning directly changes the model’s internal behavior—resulting in more consistent, efficient, and domain-aware outputs.</p>
<p>In this chapter, you’ve learned:</p>
<ul>
<li><strong>What fine-tuning is</strong> and how it builds on pre-training</li>
<li><strong>Why fine-tuning matters</strong> for cost efficiency, task specialization, control, and privacy</li>
<li><strong>How to choose a base model</strong> based on size, license, and compatibility</li>
<li><strong>How to prepare data</strong> through sourcing, cleaning, and formatting</li>
<li><strong>Which techniques to use</strong>, including full fine-tuning, LoRA, and QLoRA</li>
<li><strong>How to configure training</strong> using hyperparameters</li>
<li><strong>How to evaluate training</strong> using the loss curve to detect underfitting, overfitting, or a good fit</li>
</ul>
<p>Fine-tuning unlocks powerful customization, but it also requires aligned tools, clean data, and thoughtful training design. When done well, it can produce models that outperform much larger base models on specific tasks—using less compute and offering faster inference.</p>


</section>

</main> <!-- /main -->
<footer class="book-footer">
  <div class="footer-content">
    <div class="footer-left">
      <img src="images/strat-logo.png" alt="BYU Logo" class="footer-logo">
    </div>
    <div class="footer-right">
      <a href="https://www.scottmurff.org" target="_blank">Scott Murff</a>
      |
      <a href="https://www.linkedin.com/in/scottdmurff" target="_blank">LinkedIn</a>
    </div>
  </div>
</footer>
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./05-retrieval-augmented-generation.html" class="pagination-link" aria-label="Retrieval Augmented Generation">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Retrieval Augmented Generation</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./07-agents.html" class="pagination-link" aria-label="Agents">
        <span class="nav-page-text"><span class="chapter-title">Agents</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>